{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uh2eXRUoedSE"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Cold Diffusion Re-Implementation by Nicole Hao and Evan Vera"
      ],
      "metadata": {
        "id": "kyinUl4hUkff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our re-implementation of the model used in the paper 'Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise'. The majority of the code comes directly from the repository linked to by the researcher, though we have tried to cut down on the complexity by replacing custom defined classes with the corresponding one from PyTorch and eliminating functionality we don't need for the demo."
      ],
      "metadata": {
        "id": "oYUk8wI244yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Installs"
      ],
      "metadata": {
        "id": "K9sSpaeWUwQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31BVFUvTohIv",
        "outputId": "67968865-717d-4ca9-b729-d7a991e722f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m772.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchgeometry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3si8paMI_dq",
        "outputId": "e7a2027c-05f6-4bb8-893b-1debd4739a68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchgeometry\n",
            "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m651.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchgeometry) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchgeometry\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchgeometry-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqX03pkOKE6F",
        "outputId": "a0da10bc-f1f7-43a6-f60f-127edc4aa472"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apex\n",
            "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cryptacular (from apex)\n",
            "  Downloading cryptacular-1.6.2.tar.gz (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zope.sqlalchemy (from apex)\n",
            "  Downloading zope.sqlalchemy-3.1-py3-none-any.whl (23 kB)\n",
            "Collecting velruse>=1.0.3 (from apex)\n",
            "  Downloading velruse-1.1.1.tar.gz (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyramid>1.1.2 (from apex)\n",
            "  Downloading pyramid-2.0.2-py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyramid_mailer (from apex)\n",
            "  Downloading pyramid_mailer-0.15.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from apex) (2.31.0)\n",
            "Collecting wtforms (from apex)\n",
            "  Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wtforms-recaptcha (from apex)\n",
            "  Downloading wtforms_recaptcha-0.3.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting hupper>=1.5 (from pyramid>1.1.2->apex)\n",
            "  Downloading hupper-1.12.1-py3-none-any.whl (22 kB)\n",
            "Collecting plaster (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster-1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting plaster-pastedeploy (from pyramid>1.1.2->apex)\n",
            "  Downloading plaster_pastedeploy-1.0.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyramid>1.1.2->apex) (67.7.2)\n",
            "Collecting translationstring>=0.4 (from pyramid>1.1.2->apex)\n",
            "  Downloading translationstring-1.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting venusian>=1.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading venusian-3.1.0-py3-none-any.whl (13 kB)\n",
            "Collecting webob>=1.8.3 (from pyramid>1.1.2->apex)\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.deprecation>=3.5.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.deprecation-5.0-py3-none-any.whl (10 kB)\n",
            "Collecting zope.interface>=3.8.0 (from pyramid>1.1.2->apex)\n",
            "  Downloading zope.interface-6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.5/247.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from velruse>=1.0.3->apex) (1.3.1)\n",
            "Collecting anykeystore (from velruse>=1.0.3->apex)\n",
            "  Downloading anykeystore-0.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python3-openid (from velruse>=1.0.3->apex)\n",
            "  Downloading python3_openid-3.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pbkdf2 (from cryptacular->apex)\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting repoze.sendmail>=4.1 (from pyramid_mailer->apex)\n",
            "  Downloading repoze.sendmail-4.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transaction (from pyramid_mailer->apex)\n",
            "  Downloading transaction-4.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->apex) (2024.2.2)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from wtforms->apex) (2.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (24.0)\n",
            "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from zope.sqlalchemy->apex) (2.0.30)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.0.3)\n",
            "Collecting PasteDeploy>=2.0 (from plaster-pastedeploy->pyramid>1.1.2->apex)\n",
            "  Downloading PasteDeploy-3.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
            "Building wheels for collected packages: apex, velruse, cryptacular, anykeystore, pbkdf2\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.9.10.dev0-py3-none-any.whl size=46442 sha256=e64b20622e958e626901aa36953d588ed33f3afc5617354e63da203814fb52be\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/59/9b100fce7ebd989603b3b7a4ca259150da72c9e107fcaa2a30\n",
            "  Building wheel for velruse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for velruse: filename=velruse-1.1.1-py3-none-any.whl size=50909 sha256=3313a7b91b8bd17ec6fc1ffbd96aa420fb1425a2980bfdeca27b06bcaaa3bfed\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/f9/a4/fc4ea7b935ee9c58b9bc772cabd94f6a8560f35444097d948d\n",
            "  Building wheel for cryptacular (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cryptacular: filename=cryptacular-1.6.2-cp310-cp310-linux_x86_64.whl size=55087 sha256=00b2bc163a713e027451b156614ef0cc7fac6b33e1460ec2656962f0d6d16816\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/6e/09/a7fba517f95b2a6a36bd01b6d4f4679fa7259615a493b64b8f\n",
            "  Building wheel for anykeystore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anykeystore: filename=anykeystore-0.2-py3-none-any.whl size=16813 sha256=3b14fcefa17a568f003f40cdc0cff2434edfb773a1d0360cd90cfd931b4e544b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/9e/24/35542b7d376b53a6f8426524cc5a3f7998f975037b32d19906\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5083 sha256=6a98895065cab174c316073f7f7bb34aa862cd4e758205afee766491071521da\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/7d/8b/4269ff90fda80497ec59f6ff7d1e1596cb697c1dc8e9bbe320\n",
            "Successfully built apex velruse cryptacular anykeystore pbkdf2\n",
            "Installing collected packages: translationstring, pbkdf2, anykeystore, zope.interface, zope.deprecation, wtforms, webob, venusian, python3-openid, plaster, PasteDeploy, hupper, cryptacular, wtforms-recaptcha, transaction, plaster-pastedeploy, zope.sqlalchemy, repoze.sendmail, pyramid, velruse, pyramid_mailer, apex\n",
            "Successfully installed PasteDeploy-3.1.0 anykeystore-0.2 apex-0.9.10.dev0 cryptacular-1.6.2 hupper-1.12.1 pbkdf2-1.3 plaster-1.1.2 plaster-pastedeploy-1.0.1 pyramid-2.0.2 pyramid_mailer-0.15.1 python3-openid-3.2.0 repoze.sendmail-4.4.1 transaction-4.0 translationstring-1.4 velruse-1.1.1 venusian-3.1.0 webob-1.8.7 wtforms-3.1.2 wtforms-recaptcha-0.3.2 zope.deprecation-5.0 zope.interface-6.4 zope.sqlalchemy-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_msssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IILJseuUMcMj",
        "outputId": "d7a1693c-7eab-44cf-be72-f39b088511e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_msssim) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch_msssim) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_msssim) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n",
            "Installing collected packages: pytorch_msssim\n",
            "Successfully installed pytorch_msssim-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install comet_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0E3UCPKMnzT",
        "outputId": "8dbc75d7-5969-4f83-ba16-181bc8539e07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.42.0-py3-none-any.whl (663 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.1/663.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting everett[ini]<3.2.0,>=1.0.1 (from comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.19.2)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=0.8.0 (from comet_ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.31.0)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet_ml)\n",
            "  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.4.0,>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.7.1)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Installing collected packages: everett, wurlitzer, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.1.1\n",
            "    Uninstalling python-box-7.1.1:\n",
            "      Successfully uninstalled python-box-7.1.1\n",
            "Successfully installed comet_ml-3.42.0 configobj-5.0.8 dulwich-0.22.1 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-2.2.0 simplejson-3.19.2 wurlitzer-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tgm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNCUWwWONHcV",
        "outputId": "12ccaf48-4783-45cd-8266-27ee9333399f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tgm\n",
            "  Downloading tgm-0.0.3-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tgm) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tgm) (2.0.3)\n",
            "Collecting igraph (from tgm)\n",
            "  Downloading igraph-0.11.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2 (from igraph->tgm)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tgm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tgm) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tgm) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->tgm) (1.16.0)\n",
            "Installing collected packages: texttable, igraph, tgm\n",
            "Successfully installed igraph-0.11.5 texttable-1.7.0 tgm-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "YUskxuTjKn3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import errno\n",
        "import shutil\n",
        "import argparse\n",
        "# from comet_ml import Experiment\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "from torch.utils import data\n",
        "from pathlib import Path\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "import torchgeometry as tgm\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# from Fid import calculate_fid_given_samples (Fid score)"
      ],
      "metadata": {
        "id": "AamBN6pxZJo7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Layers"
      ],
      "metadata": {
        "id": "0AeyigdhU4s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define three layers which we will be using throughout the rest of our architecture. Firstly, we have a normalize layer, which behaves similar to a LayerNorm except it only applies it to groups of 32 channels instead of all channels like it usually does. Then, we have an up and down sample function, which we repeatedly use in our UNet."
      ],
      "metadata": {
        "id": "Z8X3uh3tU6SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalize(in_channels):\n",
        "    # How does GroupNorm work? And why are we using GroupNorm here?\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        \"\"\"\n",
        "        in_channels: the number of input channels that the feature map has. If we are using a conv layer, we need to\n",
        "        know how many channels the input feature map has.\n",
        "        with_conv: boolean, whether to use convolutions to downsample the feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "          Increases the spatial resolution (i.e., height and width) of an input feature map.\n",
        "          Input: x\n",
        "          Output:\n",
        "        \"\"\"\n",
        "        if self.with_conv:\n",
        "            # intialize padding to be added to the input tensor 'x' on each side\n",
        "            # (padding_left, padding_right, padding_top, padding_bottom) = (0,1,0,1)\n",
        "            pad = (0,1,0,1)\n",
        "\n",
        "            # pad the input tensor 'x' with the specified padding values\n",
        "            # mode=\"constant\" specifies that the padding should be a constant value.\n",
        "            # value=0 sets the padding pixels to a constant value of 0, which is typical\n",
        "            # for many padding operations since it doesn't add any new information to the input tensor.\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "\n",
        "            # Downsampling thru convolutional layer with kernel size 3 and stride 2\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            # Downsampling thru Average pooling, instead of using a conv layer\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Y26MYxGXZRBo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention block class"
      ],
      "metadata": {
        "id": "-ciyN48oZtrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In between all of our down-sampling and up-sampling in the UNet we have an attention block between adjacent connections. We implement that here as the attention block. We can observe that this is no ordinary attention, as because we are dealing with images we first need to convolve and re-shape our images until they can be transformed into our query, key, and value vectors."
      ],
      "metadata": {
        "id": "VAe8wlkgVH29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to be able to attend to our features, though this is a little\n",
        "# tricky since we have an image. Therefore, we perform some convolutions so\n",
        "# that we can actually get our Q, K, and V matrices\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "\n",
        "        # unpacks the dimensions of the query tensor into batch size (b), number of channels (c), height (h), and width (w).\n",
        "        b,c,h,w = q.shape\n",
        "\n",
        "        # reshapes the query tensor to collapse the spatial dimensions (h and w) into one,\n",
        "        # making it easier to perform matrix multiplication.\n",
        "        q = q.reshape(b,c,h*w)\n",
        "\n",
        "        # permute the dimensions of q to prepare it for matrix multiplication with k.\n",
        "        # It rearranges q so its dimensions are [batch, height*width, channels].\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "\n",
        "        # collapse the spatial dimensions (h and w) into one\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "\n",
        "        # performs batch matrix multiplication between the query and key matrices (QK^T)\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "\n",
        "        # scales the attention scores by the inverse square root of the number of channels\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_"
      ],
      "metadata": {
        "id": "kKfy4UxWZwbB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we implement a linear attention layer. Originally, I considered switching this to be the typical PyTorch implementation, however this failed."
      ],
      "metadata": {
        "id": "W4zkAkc8enGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "\n",
        "        k = k.softmax(dim = -1)\n",
        "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "sEe4XmlqemW3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Utility Functions"
      ],
      "metadata": {
        "id": "_YEWNJZlO_a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the utility functions we are going to use in our UNet. We need a more portable up-sample and down-sample, as well as the ability to embed positional information into our images so that it can detect at which stage of the distortion process its in. We also create an exponential averaging moving model module, which we will later use to compound past predictions."
      ],
      "metadata": {
        "id": "U2OTr775VknH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from apex import amp\n",
        "    APEX_AVAILABLE = True\n",
        "except:\n",
        "    APEX_AVAILABLE = False\n",
        "\n",
        "# helpers functions\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def cycle_cat(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data[0]\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "def loss_backwards(fp16, loss, optimizer, **kwargs):\n",
        "    if fp16:\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            scaled_loss.backward(**kwargs)\n",
        "    else:\n",
        "        loss.backward(**kwargs)\n",
        "\n",
        "# small helper modules\n",
        "\n",
        "# Computes an exponential moving average of our model with its past selves\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old_weight, up_weight = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old_weight, up_weight)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# Gives a positional embedding (i.e. time step embedding) so that our model\n",
        "# has some more information about where it is in the pipeline\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "# We either upsample or downsample using convolutions. Unlike previous\n",
        "# classes, we don't pad or interpolate, just use convolutions\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
        "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
        "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ],
      "metadata": {
        "id": "qNH2GxTiPHzY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Blocks"
      ],
      "metadata": {
        "id": "zWnT_U8tV9CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the blocks in our UNet."
      ],
      "metadata": {
        "id": "ejx--nfTV-U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building block modules\n",
        "# This is taken from the original Cold Diffusion paper, which itself took it\n",
        "# from a paper 'A ConvNet for the 2020s'\n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\" https://arxiv.org/abs/2201.03545 \"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim = None, mult = 2, norm = True):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_emb_dim, dim)\n",
        "        ) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding = 3, groups = dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            LayerNorm(dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding = 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding = 1)\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp):\n",
        "            assert exists(time_emb), 'time emb must be passed in'\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, 'b c -> b c 1 1')\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "OPhM7TxGRDwS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet"
      ],
      "metadata": {
        "id": "EFLXIKsdWCWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we actually implement the UNet model itself."
      ],
      "metadata": {
        "id": "dvbGIKs2WD5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels = 3,\n",
        "        with_time_emb = True,\n",
        "        residual = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.residual = residual\n",
        "        print(\"Is Time embed used ? \", with_time_emb)\n",
        "\n",
        "        dims = [channels, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if with_time_emb:\n",
        "            time_dim = dim\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPosEmb(dim),\n",
        "                nn.Linear(dim, dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim * 4, dim)\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_in, dim_out, time_emb_dim = time_dim, norm = ind != 0),\n",
        "                ConvNextBlock(dim_out, dim_out, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                Downsample(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
        "        self.mid_block2 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_out * 2, dim_in, time_emb_dim = time_dim),\n",
        "                ConvNextBlock(dim_in, dim_in, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                Upsample(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            ConvNextBlock(dim, dim),\n",
        "            nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        orig_x = x\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for convnext, convnext2, attn, downsample in self.downs:\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for convnext, convnext2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "        if self.residual:\n",
        "            return self.final_conv(x) + orig_x\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "w7XtxDMX7lBk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Diffusion Utilities"
      ],
      "metadata": {
        "id": "mP0YxnGz7dX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to implement Gaussian diffusion we need to be able to generate noisy latents as well as to give ourselves a specified noise schedule. Thus, we create some utility functions to do this."
      ],
      "metadata": {
        "id": "hA3Zbwr1WOMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gaussian diffusion trainer class\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "def noise_like(shape, device, repeat=False):\n",
        "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
        "    noise = lambda: torch.randn(shape, device=device)\n",
        "    return repeat_noise() if repeat else noise()\n",
        "\n",
        "# This is the noise schedule we use\n",
        "def cosine_beta_schedule(timesteps, s = 0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = np.linspace(0, steps, steps)\n",
        "    alphas_cumprod = np.cos(((x / steps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return np.clip(betas, a_min = 0, a_max = 0.999)"
      ],
      "metadata": {
        "id": "p-3oehUOWMFg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Diffusion Model"
      ],
      "metadata": {
        "id": "y7yqlX7xWWhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we implement a Gaussian Diffusion model which we can use to sample from using the improved algorithm presented in the Cold Diffusion paper."
      ],
      "metadata": {
        "id": "qEqbOObGWYMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class performs GaussianDiffusion, except with blurring operators instead\n",
        "# of random noise. To make sure not all images go to the same black latent\n",
        "# from blurring, we still have some randomness in how we color our images,\n",
        "# and this is where the Gaussian name comes from.\n",
        "class GaussianDiffusion(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        denoise_fn,\n",
        "        *,\n",
        "        image_size,\n",
        "        device_of_kernel,\n",
        "        channels = 3,\n",
        "        timesteps = 1000,\n",
        "        loss_type = 'l1',\n",
        "        kernel_std = 0.1,\n",
        "        kernel_size = 3,\n",
        "        blur_routine = 'Incremental',\n",
        "        train_routine = 'Final',\n",
        "        sampling_routine='default',\n",
        "        discrete=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.image_size = image_size\n",
        "        self.denoise_fn = denoise_fn\n",
        "        self.device_of_kernel = device_of_kernel\n",
        "\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        self.loss_type = loss_type\n",
        "        self.kernel_std = kernel_std\n",
        "        self.kernel_size = kernel_size\n",
        "        self.blur_routine = blur_routine\n",
        "\n",
        "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
        "        self.gaussian_kernels = nn.ModuleList(self.get_kernels())\n",
        "        self.train_routine = train_routine\n",
        "        self.sampling_routine = sampling_routine\n",
        "        self.discrete=discrete\n",
        "\n",
        "    def blur(self, dims, std):\n",
        "        return tgm.image.get_gaussian_kernel2d(dims, std)\n",
        "\n",
        "    def get_conv(self, dims, std, mode='circular'):\n",
        "        kernel = self.blur(dims, std)\n",
        "        conv = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=dims, padding=int((dims[0]-1)/2), padding_mode=mode,\n",
        "                         bias=False, groups=self.channels)\n",
        "        with torch.no_grad():\n",
        "            kernel = torch.unsqueeze(kernel, 0)\n",
        "            kernel = torch.unsqueeze(kernel, 0)\n",
        "            kernel = kernel.repeat(self.channels, 1, 1, 1)\n",
        "            conv.weight = nn.Parameter(kernel)\n",
        "\n",
        "        return conv\n",
        "\n",
        "    def get_kernels(self):\n",
        "        kernels = []\n",
        "        for i in range(self.num_timesteps):\n",
        "            if self.blur_routine == 'Incremental':\n",
        "                kernels.append(self.get_conv((self.kernel_size, self.kernel_size), (self.kernel_std*(i+1), self.kernel_std*(i+1)) ) )\n",
        "            elif self.blur_routine == 'Constant':\n",
        "                kernels.append(self.get_conv((self.kernel_size, self.kernel_size), (self.kernel_std, self.kernel_std) ) )\n",
        "            elif self.blur_routine == 'Constant_reflect':\n",
        "                kernels.append(self.get_conv((self.kernel_size, self.kernel_size), (self.kernel_std, self.kernel_std), mode='reflect') )\n",
        "            elif self.blur_routine == 'Exponential_reflect':\n",
        "                ks = self.kernel_size\n",
        "                kstd = np.exp(self.kernel_std * i)\n",
        "                kernels.append(self.get_conv((ks, ks), (kstd, kstd), mode='reflect'))\n",
        "            elif self.blur_routine == 'Exponential':\n",
        "                ks = self.kernel_size\n",
        "                kstd = np.exp(self.kernel_std * i)\n",
        "                kernels.append(self.get_conv((ks, ks), (kstd, kstd)))\n",
        "            elif self.blur_routine == 'Individual_Incremental':\n",
        "                ks = 2*i+1\n",
        "                kstd = 2*ks\n",
        "                kernels.append(self.get_conv((ks, ks), (kstd, kstd)))\n",
        "            elif self.blur_routine == 'Special_6_routine':\n",
        "                ks = 11\n",
        "                kstd = i/100 + 0.35\n",
        "                kernels.append(self.get_conv((ks, ks), (kstd, kstd), mode='reflect'))\n",
        "\n",
        "        return kernels\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size = 16, img=None, t=None):\n",
        "\n",
        "        self.denoise_fn.eval()\n",
        "\n",
        "        if t==None:\n",
        "            t=self.num_timesteps\n",
        "\n",
        "        if self.blur_routine == 'Individual_Incremental':\n",
        "            img = self.gaussian_kernels[t-1](img)\n",
        "\n",
        "        else:\n",
        "            for i in range(t):\n",
        "                with torch.no_grad():\n",
        "                    img = self.gaussian_kernels[i](img)\n",
        "\n",
        "        orig_mean = torch.mean(img, [2, 3], keepdim=True)\n",
        "        print(orig_mean.squeeze()[0])\n",
        "\n",
        "        temp = img\n",
        "        if self.discrete:\n",
        "            img = torch.mean(img, [2, 3], keepdim=True)\n",
        "            img = img.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "\n",
        "        # 3(2), 2(1), 1(0)\n",
        "        xt = img\n",
        "        direct_recons = None\n",
        "        while(t):\n",
        "            step = torch.full((batch_size,), t - 1, dtype=torch.long).cuda()\n",
        "            x = self.denoise_fn(img, step)\n",
        "\n",
        "            if self.train_routine == 'Final':\n",
        "                if direct_recons == None:\n",
        "                    direct_recons = x\n",
        "\n",
        "                if self.sampling_routine == 'default':\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        x = self.gaussian_kernels[t - 2](x)\n",
        "                    else:\n",
        "                        for i in range(t-1):\n",
        "                            with torch.no_grad():\n",
        "                                x = self.gaussian_kernels[i](x)\n",
        "\n",
        "                elif self.sampling_routine == 'x0_step_down':\n",
        "                    x_times = x\n",
        "                    for i in range(t):\n",
        "                        with torch.no_grad():\n",
        "                            x_times = self.gaussian_kernels[i](x_times)\n",
        "                            if self.discrete:\n",
        "                                if i == (self.num_timesteps - 1):\n",
        "                                    x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                                    x_times = x_times.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "\n",
        "                    x_times_sub_1 = x\n",
        "                    for i in range(t - 1):\n",
        "                        with torch.no_grad():\n",
        "                            x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                    x = img - x_times + x_times_sub_1\n",
        "            img = x\n",
        "            t = t - 1\n",
        "        self.denoise_fn.train()\n",
        "        return xt, direct_recons, img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def all_sample(self, batch_size=16, img=None, t=None, times=None, eval=True):\n",
        "\n",
        "        if eval:\n",
        "            self.denoise_fn.eval()\n",
        "\n",
        "        if t == None:\n",
        "            t = self.num_timesteps\n",
        "        if times == None:\n",
        "            times = t\n",
        "\n",
        "        if self.blur_routine == 'Individual_Incremental':\n",
        "            img = self.gaussian_kernels[t - 1](img)\n",
        "        else:\n",
        "            for i in range(t):\n",
        "                with torch.no_grad():\n",
        "                    img = self.gaussian_kernels[i](img)\n",
        "\n",
        "        X_0s = []\n",
        "        X_ts = []\n",
        "        temp = img\n",
        "        if self.discrete:\n",
        "            img = torch.mean(img, [2, 3], keepdim=True)\n",
        "            img = img.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "            noise = torch.randn_like(img) * 0.001\n",
        "            img = img + noise\n",
        "\n",
        "        # 3(2), 2(1), 1(0)\n",
        "        while (times):\n",
        "            step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda()\n",
        "            x = self.denoise_fn(img, step)\n",
        "            X_0s.append(x)\n",
        "            X_ts.append(img)\n",
        "\n",
        "            if self.train_routine == 'Final':\n",
        "                if self.sampling_routine == 'default':\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        if times-2 >= 0:\n",
        "                            x = self.gaussian_kernels[times - 2](img)\n",
        "                    else:\n",
        "                        x_times_sub_1 = x\n",
        "                        for i in range(times-1):\n",
        "                            with torch.no_grad():\n",
        "                                x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                        x = x_times_sub_1\n",
        "\n",
        "                elif self.sampling_routine == 'x0_step_down':\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        if times-2 >= 0:\n",
        "                            x = self.gaussian_kernels[times - 2](img)\n",
        "                    else:\n",
        "                        x_times = x\n",
        "                        for i in range(times):\n",
        "                            with torch.no_grad():\n",
        "                                x_times = self.gaussian_kernels[i](x_times)\n",
        "                                if self.discrete:\n",
        "                                    if i == (self.num_timesteps - 1):\n",
        "                                        x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                                        x_times = x_times.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "\n",
        "\n",
        "                        x_times_sub_1 = x\n",
        "                        for i in range(times - 1):\n",
        "                            with torch.no_grad():\n",
        "                                x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                        x = img - x_times + x_times_sub_1\n",
        "                        #x = x - (noise/self.num_timesteps)\n",
        "\n",
        "\n",
        "\n",
        "            img = x\n",
        "            times = times - 1\n",
        "\n",
        "        if self.discrete:\n",
        "            img = img - noise\n",
        "        X_0s.append(img)\n",
        "\n",
        "        self.denoise_fn.train()\n",
        "        return X_0s, X_ts\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_and_backward(self, batch_size=16, img=None, noise_level=0, t=None, times=None, eval=True):\n",
        "\n",
        "        if eval:\n",
        "            self.denoise_fn.eval()\n",
        "\n",
        "        if t == None:\n",
        "            t = self.num_timesteps\n",
        "        if times == None:\n",
        "            times = t\n",
        "\n",
        "        Forward = []\n",
        "        Forward.append(img)\n",
        "\n",
        "        if self.blur_routine == 'Individual_Incremental':\n",
        "            img = self.gaussian_kernels[t - 1](img)\n",
        "        else:\n",
        "            for i in range(t):\n",
        "                with torch.no_grad():\n",
        "                    img = self.gaussian_kernels[i](img)\n",
        "                    Forward.append(img)\n",
        "\n",
        "        Backward = []\n",
        "        temp = img\n",
        "        if self.discrete:\n",
        "            img = torch.mean(img, [2, 3], keepdim=True)\n",
        "            img = img.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "            noise = torch.randn_like(img) * noise_level\n",
        "            img = img + noise\n",
        "\n",
        "        # 3(2), 2(1), 1(0)\n",
        "        while (times):\n",
        "            print(times)\n",
        "            step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda()\n",
        "            x = self.denoise_fn(img, step)\n",
        "            Backward.append(img)\n",
        "\n",
        "            if self.train_routine == 'Final':\n",
        "                if self.sampling_routine == 'default':\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        if times - 2 >= 0:\n",
        "                            x = self.gaussian_kernels[times - 2](img)\n",
        "                    else:\n",
        "                        x_times_sub_1 = x\n",
        "                        for i in range(times - 1):\n",
        "                            with torch.no_grad():\n",
        "                                x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                        x = x_times_sub_1\n",
        "\n",
        "                elif self.sampling_routine == 'x0_step_down':\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        if times - 2 >= 0:\n",
        "                            x = self.gaussian_kernels[times - 2](img)\n",
        "                    else:\n",
        "                        x_times = x\n",
        "                        for i in range(times):\n",
        "                            with torch.no_grad():\n",
        "                                x_times = self.gaussian_kernels[i](x_times)\n",
        "                                if self.discrete:\n",
        "                                    if i == (self.num_timesteps - 1):\n",
        "                                        x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                                        x_times = x_times.expand(temp.shape[0], temp.shape[1], temp.shape[2],\n",
        "                                                                 temp.shape[3])\n",
        "\n",
        "                        x_times_sub_1 = x\n",
        "                        for i in range(times - 1):\n",
        "                            with torch.no_grad():\n",
        "                                x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                        x = img - x_times + x_times_sub_1\n",
        "                        # x = x - (noise/self.num_timesteps)\n",
        "\n",
        "            img = x\n",
        "            times = times - 1\n",
        "\n",
        "\n",
        "        return Forward, Backward, img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_and_backward_2(self, batch_size=16, img=None, noise_level=0, eval=True):\n",
        "\n",
        "        if eval:\n",
        "            self.denoise_fn.eval()\n",
        "\n",
        "\n",
        "        times = self.num_timesteps\n",
        "\n",
        "        Forward = []\n",
        "        orig_img = img\n",
        "        Forward.append(img)\n",
        "\n",
        "        for i in range(times):\n",
        "            with torch.no_grad():\n",
        "                img = self.gaussian_kernels[i](img)\n",
        "                Forward.append(img)\n",
        "\n",
        "        Backward_1 = []\n",
        "        temp = img\n",
        "        if self.discrete:\n",
        "            img = torch.mean(img, [2, 3], keepdim=True)\n",
        "            img = img.expand(temp.shape[0], temp.shape[1], temp.shape[2], temp.shape[3])\n",
        "            noise = torch.randn_like(img) * noise_level\n",
        "            img = img + noise\n",
        "        last_img = img\n",
        "\n",
        "        while (times):\n",
        "            print(times)\n",
        "            step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda()\n",
        "            x = self.denoise_fn(img, step)\n",
        "            Backward_1.append(img)\n",
        "\n",
        "            x_times = x\n",
        "            for i in range(times):\n",
        "                with torch.no_grad():\n",
        "                    x_times = self.gaussian_kernels[i](x_times)\n",
        "                    if self.discrete:\n",
        "                        if i == (self.num_timesteps - 1):\n",
        "                            x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                            x_times = x_times.expand(temp.shape[0], temp.shape[1], temp.shape[2],\n",
        "                                                     temp.shape[3])\n",
        "\n",
        "            x_times_sub_1 = x\n",
        "            for i in range(times - 1):\n",
        "                with torch.no_grad():\n",
        "                    x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "            x = img - img + x_times_sub_1\n",
        "\n",
        "\n",
        "\n",
        "            img = x\n",
        "            times = times - 1\n",
        "\n",
        "        img_1 = img\n",
        "\n",
        "        times = self.num_timesteps\n",
        "        img = last_img\n",
        "        Backward_2 = []\n",
        "\n",
        "        while (times):\n",
        "            print(times)\n",
        "            step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda()\n",
        "            x = self.denoise_fn(img, step)\n",
        "            Backward_2.append(img)\n",
        "\n",
        "            x_times = x\n",
        "            for i in range(times):\n",
        "                with torch.no_grad():\n",
        "                    x_times = self.gaussian_kernels[i](x_times)\n",
        "                    if self.discrete:\n",
        "                        if i == (self.num_timesteps - 1):\n",
        "                            x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                            x_times = x_times.expand(temp.shape[0], temp.shape[1], temp.shape[2],\n",
        "                                                     temp.shape[3])\n",
        "\n",
        "            x_times_sub_1 = x\n",
        "            for i in range(times - 1):\n",
        "                with torch.no_grad():\n",
        "                    x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "            x = img - x_times + x_times_sub_1\n",
        "\n",
        "            img = x\n",
        "            times = times - 1\n",
        "\n",
        "        img_2 = img\n",
        "\n",
        "        return Forward, Backward_1, Backward_2, img_1, img_2\n",
        "\n",
        "    def q_sample(self, x_start, t):\n",
        "        # So at present we will for each batch blur it till the max in t.\n",
        "        # And save it. And then use t to pull what I need. It is nothing but series of convolutions anyway.\n",
        "        # Remember to do convs without torch.grad\n",
        "        max_iters = torch.max(t)\n",
        "        all_blurs = []\n",
        "        x = x_start\n",
        "        for i in range(max_iters+1):\n",
        "            with torch.no_grad():\n",
        "                x = self.gaussian_kernels[i](x)\n",
        "                if self.discrete:\n",
        "                    if i == (self.num_timesteps-1):\n",
        "                        x = torch.mean(x, [2, 3], keepdim=True)\n",
        "                        x = x.expand(x_start.shape[0], x_start.shape[1], x_start.shape[2], x_start.shape[3])\n",
        "                all_blurs.append(x)\n",
        "\n",
        "        all_blurs = torch.stack(all_blurs)\n",
        "\n",
        "        choose_blur = []\n",
        "        # step is batch size as well so for the 49th step take the step(batch_size)\n",
        "        for step in range(t.shape[0]):\n",
        "            if step != -1:\n",
        "                choose_blur.append(all_blurs[t[step], step])\n",
        "            else:\n",
        "                choose_blur.append(x_start[step])\n",
        "\n",
        "        choose_blur = torch.stack(choose_blur)\n",
        "        if self.discrete:\n",
        "            choose_blur = (choose_blur + 1) * 0.5\n",
        "            choose_blur = (choose_blur * 255)\n",
        "            choose_blur = choose_blur.int().float() / 255\n",
        "            choose_blur = choose_blur * 2 - 1\n",
        "        #choose_blur = all_blurs\n",
        "        return choose_blur\n",
        "\n",
        "\n",
        "    def p_losses(self, x_start, t):\n",
        "        b, c, h, w = x_start.shape\n",
        "        if self.train_routine == 'Final':\n",
        "            x_blur = self.q_sample(x_start=x_start, t=t)\n",
        "            x_recon = self.denoise_fn(x_blur, t)\n",
        "            if self.loss_type == 'l1':\n",
        "                loss = (x_start - x_recon).abs().mean()\n",
        "            elif self.loss_type == 'l2':\n",
        "                loss = F.mse_loss(x_start, x_recon)\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
        "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "        return self.p_losses(x, t, *args, **kwargs)"
      ],
      "metadata": {
        "id": "moAR2KIgQpAv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Augmentations"
      ],
      "metadata": {
        "id": "qrwWy_W8BvsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define two classes which can hold our training data and perform various augmentations to it."
      ],
      "metadata": {
        "id": "bP1442tfYRo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Here, we create a custom dataset loader that can transform our images,\n",
        "# giving us more training data\n",
        "class Dataset_Aug1(data.Dataset):\n",
        "    def __init__(self, folder, image_size, exts = ['jpg', 'jpeg', 'png']):\n",
        "        super().__init__()\n",
        "        self.folder = folder\n",
        "        self.image_size = image_size\n",
        "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((int(image_size*1.12), int(image_size*1.12))),\n",
        "            transforms.RandomCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)"
      ],
      "metadata": {
        "id": "gauGS-gRQO2b"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class holds our data\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, folder, image_size, exts=['jpg', 'jpeg', 'png']):\n",
        "        super().__init__()\n",
        "        self.folder = folder\n",
        "        self.image_size = image_size\n",
        "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((int(image_size*1.12), int(image_size*1.12))),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n"
      ],
      "metadata": {
        "id": "sPdYU65-QMMG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer Class"
      ],
      "metadata": {
        "id": "WBu7Zsl95g5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class trains our UNet model to be able to reverse the noising process of our Gaussian Diffusion model. It also gives us handy functions for sampling from our MNIST dataset by repeatedly blurring an image and then trying to revert the process."
      ],
      "metadata": {
        "id": "ai4IHhv_YjtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer class\n",
        "def create_folder(path):\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError as exc:\n",
        "        if exc.errno != errno.EEXIST:\n",
        "            raise\n",
        "        pass\n",
        "\n",
        "from collections import OrderedDict\n",
        "def remove_data_parallel(old_state_dict):\n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    for k, v in old_state_dict.items():\n",
        "        name = k.replace('.module', '')  # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "\n",
        "    return new_state_dict\n",
        "\n",
        "def adjust_data_parallel(old_state_dict):\n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    for k, v in old_state_dict.items():\n",
        "        name = k.replace('denoise_fn.module', 'module.denoise_fn')  # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "\n",
        "    return new_state_dict"
      ],
      "metadata": {
        "id": "eVF43npuP_KT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import utils\n",
        "\n",
        "# This class is responsible for actuall training our model to be able\n",
        "# to predict the noise generated by our GaussianDiffusion model (well,\n",
        "# not really a model but you get the point)\n",
        "class Trainer(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        folder,\n",
        "        *,\n",
        "        ema_decay = 0.995,\n",
        "        image_size = 128,\n",
        "        train_batch_size = 32,\n",
        "        train_lr = 2e-5,\n",
        "        train_num_steps = 100000,\n",
        "        gradient_accumulate_every = 2,\n",
        "        fp16 = False,\n",
        "        step_start_ema = 2000,\n",
        "        update_ema_every = 10,\n",
        "        save_and_sample_every = 1000,\n",
        "        results_folder = './results',\n",
        "        load_path = None,\n",
        "        dataset = None,\n",
        "        shuffle=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = diffusion_model\n",
        "        self.ema = EMA(ema_decay)\n",
        "        self.ema_model = copy.deepcopy(self.model)\n",
        "        self.update_ema_every = update_ema_every\n",
        "\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.image_size = image_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "        self.train_num_steps = train_num_steps\n",
        "\n",
        "        if dataset == 'mnist' or dataset == 'cifar10' or dataset == 'flower' or dataset == 'celebA' or dataset == 'AFHQ':\n",
        "            print(dataset, \"DA used\")\n",
        "            self.ds = Dataset_Aug1(folder, image_size)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=train_batch_size, shuffle=shuffle, pin_memory=True, num_workers=8,\n",
        "                                drop_last=True))\n",
        "\n",
        "        elif dataset == 'LSUN_train':\n",
        "            print(dataset, \"DA used\")\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((int(image_size * 1.12), int(image_size * 1.12))),\n",
        "                transforms.RandomCrop(image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "            ])\n",
        "            self.ds = torchvision.datasets.LSUN(root=folder, classes=['church_outdoor_train'], transform=transform)\n",
        "            self.dl = cycle_cat(data.DataLoader(self.ds, batch_size=train_batch_size, shuffle=shuffle, pin_memory=True, num_workers=16,\n",
        "                                drop_last=True))\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(dataset)\n",
        "            self.ds = Dataset(folder, image_size)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=train_batch_size, shuffle=shuffle, pin_memory=True, num_workers=16,\n",
        "                                drop_last=True))\n",
        "\n",
        "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr)\n",
        "        self.step = 0\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok = True)\n",
        "\n",
        "        self.fp16 = fp16\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if load_path != None:\n",
        "            self.load(load_path)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.ema_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def step_ema(self):\n",
        "        if self.step < self.step_start_ema:\n",
        "            self.reset_parameters()\n",
        "            return\n",
        "        self.ema.update_model_average(self.ema_model, self.model)\n",
        "\n",
        "    def save(self, itrs=None):\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.model.state_dict(),\n",
        "            'ema': self.ema_model.state_dict()\n",
        "        }\n",
        "        if itrs is None:\n",
        "            torch.save(data, str(self.results_folder / f'model.pt'))\n",
        "        else:\n",
        "            torch.save(data, str(self.results_folder / f'model_{itrs}.pt'))\n",
        "\n",
        "    def load(self, load_path):\n",
        "        print(\"Loading : \", load_path)\n",
        "        data = torch.load(load_path)\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.model.load_state_dict(data['model'])\n",
        "        self.ema_model.load_state_dict(data['ema'])\n",
        "\n",
        "\n",
        "    def add_title(self, path, title):\n",
        "\n",
        "        import cv2\n",
        "        import numpy as np\n",
        "\n",
        "        img1 = cv2.imread(path)\n",
        "\n",
        "        # --- Here I am creating the border---\n",
        "        black = [0, 0, 0]  # ---Color of the border---\n",
        "        constant = cv2.copyMakeBorder(img1, 10, 10, 10, 10, cv2.BORDER_CONSTANT, value=black)\n",
        "        height = 20\n",
        "        violet = np.zeros((height, constant.shape[1], 3), np.uint8)\n",
        "        violet[:] = (255, 0, 180)\n",
        "\n",
        "        vcat = cv2.vconcat((violet, constant))\n",
        "\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "        cv2.putText(vcat, str(title), (violet.shape[1] // 2, height-2), font, 0.5, (0, 0, 0), 1, 0)\n",
        "        cv2.imwrite(path, vcat)\n",
        "\n",
        "    def train(self):\n",
        "        # training loop\n",
        "\n",
        "        backwards = partial(loss_backwards, self.fp16)\n",
        "\n",
        "        acc_loss = 0\n",
        "        while self.step < self.train_num_steps:\n",
        "            u_loss = 0\n",
        "            for i in range(self.gradient_accumulate_every):\n",
        "                data = next(self.dl).cuda()\n",
        "                loss = torch.mean(self.model(data)) # change for DP\n",
        "                if self.step % 100 == 0:\n",
        "                    print(f'{self.step}: {loss.item()}')\n",
        "                u_loss += loss.item()\n",
        "                backwards(loss / self.gradient_accumulate_every, self.opt)\n",
        "\n",
        "            acc_loss = acc_loss + (u_loss/self.gradient_accumulate_every)\n",
        "\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "            if self.step % self.update_ema_every == 0:\n",
        "                self.step_ema()\n",
        "\n",
        "            if self.step != 0 and self.step % self.save_and_sample_every == 0:\n",
        "                milestone = self.step // self.save_and_sample_every\n",
        "                batches = self.batch_size\n",
        "                og_img = next(self.dl).cuda()\n",
        "                xt, direct_recons, all_images = self.ema_model.module.sample(batch_size=batches, img=og_img) # change for DP\n",
        "\n",
        "                og_img = (og_img + 1) * 0.5\n",
        "                utils.save_image(og_img, str(self.results_folder / f'sample-og-{milestone}.png'), nrow=6)\n",
        "\n",
        "                all_images = (all_images + 1) * 0.5\n",
        "                utils.save_image(all_images, str(self.results_folder / f'sample-recon-{milestone}.png'), nrow = 6)\n",
        "\n",
        "                direct_recons = (direct_recons + 1) * 0.5\n",
        "                utils.save_image(direct_recons, str(self.results_folder / f'sample-direct_recons-{milestone}.png'), nrow=6)\n",
        "\n",
        "                xt = (xt + 1) * 0.5\n",
        "                utils.save_image(xt, str(self.results_folder / f'sample-xt-{milestone}.png'),\n",
        "                                 nrow=6)\n",
        "\n",
        "                acc_loss = acc_loss/(self.save_and_sample_every+1)\n",
        "                print(f'Mean of last {self.step}: {acc_loss}')\n",
        "                acc_loss=0\n",
        "\n",
        "                self.save()\n",
        "                if self.step % (self.save_and_sample_every * 100) == 0:\n",
        "                    self.save(self.step)\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "        print('training completed')\n",
        "\n",
        "\n",
        "    def test_from_data(self, extra_path, s_times=None):\n",
        "        batches = self.batch_size\n",
        "        og_img = next(self.dl).cuda()\n",
        "        X_0s, X_ts = self.ema_model.module.all_sample(batch_size=batches, img=og_img, times=s_times) # change for DP\n",
        "\n",
        "        og_img = (og_img + 1) * 0.5\n",
        "        utils.save_image(og_img, str(self.results_folder / f'og-{extra_path}.png'), nrow=6)\n",
        "\n",
        "        import imageio\n",
        "        frames_t = []\n",
        "        frames_0 = []\n",
        "\n",
        "        for i in range(len(X_0s)):\n",
        "            print(i)\n",
        "\n",
        "            x_0 = X_0s[i]\n",
        "            x_0 = (x_0 + 1) * 0.5\n",
        "            utils.save_image(x_0, str(self.results_folder / f'sample-{i}-{extra_path}-x0.png'), nrow=6)\n",
        "            self.add_title(str(self.results_folder / f'sample-{i}-{extra_path}-x0.png'), str(i))\n",
        "            frames_0.append(imageio.imread(str(self.results_folder / f'sample-{i}-{extra_path}-x0.png')))\n",
        "\n",
        "            x_t = X_ts[i]\n",
        "            all_images = (x_t + 1) * 0.5\n",
        "            utils.save_image(all_images, str(self.results_folder / f'sample-{i}-{extra_path}-xt.png'), nrow=6)\n",
        "            self.add_title(str(self.results_folder / f'sample-{i}-{extra_path}-xt.png'), str(i))\n",
        "            frames_t.append(imageio.imread(str(self.results_folder / f'sample-{i}-{extra_path}-xt.png')))\n",
        "\n",
        "        imageio.mimsave(str(self.results_folder / f'Gif-{extra_path}-x0.gif'), frames_0)\n",
        "        imageio.mimsave(str(self.results_folder / f'Gif-{extra_path}-xt.gif'), frames_t)\n",
        "\n",
        "\n",
        "    def paper_showing_diffusion_images_cover_page_both_sampling(self):\n",
        "\n",
        "        import cv2\n",
        "        cnt = 0\n",
        "        to_show = [2, 4, 8, 16, 32, 64, 128, 192, 256]\n",
        "\n",
        "        for i in range(50):\n",
        "            batches = self.batch_size\n",
        "            og_img = next(self.dl).cuda()\n",
        "            print(og_img.shape)\n",
        "\n",
        "            Forward, Backward_1, Backward_2, final_all_1, final_all_2 = self.ema_model.module.forward_and_backward_2(batch_size=batches, img=og_img, noise_level=0.000)\n",
        "            og_img = (og_img + 1) * 0.5\n",
        "            final_all_1 = (final_all_1 + 1) * 0.5\n",
        "            final_all_2 = (final_all_2 + 1) * 0.5\n",
        "\n",
        "\n",
        "            for k in range(Forward[0].shape[0]):\n",
        "                l_1 = []\n",
        "                l_2 = []\n",
        "\n",
        "                utils.save_image(og_img[k], str(self.results_folder / f'og_img_{cnt}.png'), nrow=1)\n",
        "                start = cv2.imread(f'{self.results_folder}/og_img_{cnt}.png')\n",
        "                l_1.append(start)\n",
        "                l_2.append(start)\n",
        "\n",
        "                for j in range(len(Forward)):\n",
        "                    x_t = Forward[j][k]\n",
        "                    x_t = (x_t + 1) * 0.5\n",
        "                    utils.save_image(x_t, str(self.results_folder / f'temp.png'), nrow=1)\n",
        "                    x_t = cv2.imread(f'{self.results_folder}/temp.png')\n",
        "                    if j in to_show:\n",
        "                        l_1.append(x_t)\n",
        "                        l_2.append(x_t)\n",
        "\n",
        "                for j in range(len(Backward_1)):\n",
        "                    x_t = Backward_1[j][k]\n",
        "                    x_t = (x_t + 1) * 0.5\n",
        "                    utils.save_image(x_t, str(self.results_folder / f'temp.png'), nrow=1)\n",
        "                    x_t = cv2.imread(f'{self.results_folder}/temp.png')\n",
        "                    if (len(Backward_1) - j) in to_show:\n",
        "                        l_1.append(x_t)\n",
        "\n",
        "                for j in range(len(Backward_2)):\n",
        "                    x_t = Backward_2[j][k]\n",
        "                    x_t = (x_t + 1) * 0.5\n",
        "                    utils.save_image(x_t, str(self.results_folder / f'temp.png'), nrow=1)\n",
        "                    x_t = cv2.imread(f'{self.results_folder}/temp.png')\n",
        "                    if (len(Backward_2) - j) in to_show:\n",
        "                        l_2.append(x_t)\n",
        "\n",
        "\n",
        "                utils.save_image(final_all_1[k], str(self.results_folder / f'final_1_{cnt}.png'), nrow=1)\n",
        "                final_1 = cv2.imread(f'{self.results_folder}/final_1_{cnt}.png')\n",
        "                l_1.append(final_1)\n",
        "\n",
        "                utils.save_image(final_all_2[k], str(self.results_folder / f'final_2_{cnt}.png'), nrow=1)\n",
        "                final_2 = cv2.imread(f'{self.results_folder}/final_2_{cnt}.png')\n",
        "                l_2.append(final_2)\n",
        "\n",
        "\n",
        "                im_h = cv2.hconcat(l_1)\n",
        "                cv2.imwrite(f'{self.results_folder}/all_1_{cnt}.png', im_h)\n",
        "\n",
        "                im_h = cv2.hconcat(l_2)\n",
        "                cv2.imwrite(f'{self.results_folder}/all_2_{cnt}.png', im_h)\n",
        "\n",
        "                cnt+=1\n",
        "\n",
        "\n",
        "    def paper_showing_diffusion_images_cover_page(self):\n",
        "\n",
        "        import cv2\n",
        "        cnt = 0\n",
        "        to_show = [2, 4, 8, 16, 32, 64, 128, 192, 256]\n",
        "\n",
        "        for i in range(50):\n",
        "            batches = self.batch_size\n",
        "            og_img = next(self.dl).cuda()\n",
        "            print(og_img.shape)\n",
        "\n",
        "            Forward, Backward, final_all = self.ema_model.module.forward_and_backward(batch_size=batches, img=og_img, noise_level=0.002)\n",
        "            og_img = (og_img + 1) * 0.5\n",
        "            final_all = (final_all + 1) * 0.5\n",
        "\n",
        "            for k in range(Forward[0].shape[0]):\n",
        "                l = []\n",
        "\n",
        "                utils.save_image(og_img[k], str(self.results_folder / f'og_img_{cnt}.png'), nrow=1)\n",
        "                start = cv2.imread(f'{self.results_folder}/og_img_{cnt}.png')\n",
        "                l.append(start)\n",
        "\n",
        "                for j in range(len(Forward)):\n",
        "                    x_t = Forward[j][k]\n",
        "                    x_t = (x_t + 1) * 0.5\n",
        "                    utils.save_image(x_t, str(self.results_folder / f'temp.png'), nrow=1)\n",
        "                    x_t = cv2.imread(f'{self.results_folder}/temp.png')\n",
        "                    if j in to_show:\n",
        "                        l.append(x_t)\n",
        "\n",
        "                for j in range(len(Backward)):\n",
        "                    x_t = Backward[j][k]\n",
        "                    x_t = (x_t + 1) * 0.5\n",
        "                    utils.save_image(x_t, str(self.results_folder / f'temp.png'), nrow=1)\n",
        "                    x_t = cv2.imread(f'{self.results_folder}/temp.png')\n",
        "                    if (len(Backward) - j) in to_show:\n",
        "                        l.append(x_t)\n",
        "\n",
        "\n",
        "                utils.save_image(final_all[k], str(self.results_folder / f'final_{cnt}.png'), nrow=1)\n",
        "                final = cv2.imread(f'{self.results_folder}/final_{cnt}.png')\n",
        "                l.append(final)\n",
        "\n",
        "\n",
        "                im_h = cv2.hconcat(l)\n",
        "                cv2.imwrite(f'{self.results_folder}/all_{cnt}.png', im_h)\n",
        "\n",
        "                cnt+=1\n",
        "\n",
        "    def save_training_data(self):\n",
        "        dataset = self.ds\n",
        "        create_folder(f'{self.results_folder}/')\n",
        "\n",
        "        print(len(dataset))\n",
        "        for idx in range(len(dataset)):\n",
        "            img = dataset[idx]\n",
        "            img = (img + 1) * 0.5\n",
        "            utils.save_image(img, str(f'{self.results_folder}/' + f'{idx}.png'))\n",
        "            if idx%1000 == 0:\n",
        "                print(idx)"
      ],
      "metadata": {
        "id": "PVt4378kP5gf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting MNIST data"
      ],
      "metadata": {
        "id": "uh2eXRUoedSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we download the MNIST data locally so that we can train with it."
      ],
      "metadata": {
        "id": "8dMxfNggY2x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.MNIST(\n",
        "            root='content/MyDrive/CS4782/Data/MNIST', train=True, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXYDMUWveqpo",
        "outputId": "ace03b30-48c5-4df3-9149-4daa7163c7ab",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 30510275.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting content/MyDrive/CS4782/Data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1069867.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting content/MyDrive/CS4782/Data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9028956.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting content/MyDrive/CS4782/Data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5563822.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting content/MyDrive/CS4782/Data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to content/MyDrive/CS4782/Data/MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folder(path):\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError as exc:\n",
        "        if exc.errno != errno.EEXIST:\n",
        "            raise\n",
        "        pass\n",
        "\n",
        "def del_folder(path):\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "    except OSError as exc:\n",
        "        pass\n",
        "\n",
        "root = 'content/MyDrive/CS4782/Data/MNIST'\n",
        "\n",
        "# create_folder(root)\n",
        "\n",
        "# Saving data locally and labeling\n",
        "# 10 folders\n",
        "for i in range(10):\n",
        "    lable_root = root + str(i) + '/'\n",
        "    create_folder(lable_root)\n",
        "\n",
        "for idx in range(len(trainset)):\n",
        "    img, label = trainset[idx]\n",
        "    print(idx)\n",
        "    img.save(root + str(label) + '/' + str(idx) + '.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-UYknvNefER",
        "outputId": "e0b3becc-d6b9-4170-c343-8123a9d76d8c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "55000\n",
            "55001\n",
            "55002\n",
            "55003\n",
            "55004\n",
            "55005\n",
            "55006\n",
            "55007\n",
            "55008\n",
            "55009\n",
            "55010\n",
            "55011\n",
            "55012\n",
            "55013\n",
            "55014\n",
            "55015\n",
            "55016\n",
            "55017\n",
            "55018\n",
            "55019\n",
            "55020\n",
            "55021\n",
            "55022\n",
            "55023\n",
            "55024\n",
            "55025\n",
            "55026\n",
            "55027\n",
            "55028\n",
            "55029\n",
            "55030\n",
            "55031\n",
            "55032\n",
            "55033\n",
            "55034\n",
            "55035\n",
            "55036\n",
            "55037\n",
            "55038\n",
            "55039\n",
            "55040\n",
            "55041\n",
            "55042\n",
            "55043\n",
            "55044\n",
            "55045\n",
            "55046\n",
            "55047\n",
            "55048\n",
            "55049\n",
            "55050\n",
            "55051\n",
            "55052\n",
            "55053\n",
            "55054\n",
            "55055\n",
            "55056\n",
            "55057\n",
            "55058\n",
            "55059\n",
            "55060\n",
            "55061\n",
            "55062\n",
            "55063\n",
            "55064\n",
            "55065\n",
            "55066\n",
            "55067\n",
            "55068\n",
            "55069\n",
            "55070\n",
            "55071\n",
            "55072\n",
            "55073\n",
            "55074\n",
            "55075\n",
            "55076\n",
            "55077\n",
            "55078\n",
            "55079\n",
            "55080\n",
            "55081\n",
            "55082\n",
            "55083\n",
            "55084\n",
            "55085\n",
            "55086\n",
            "55087\n",
            "55088\n",
            "55089\n",
            "55090\n",
            "55091\n",
            "55092\n",
            "55093\n",
            "55094\n",
            "55095\n",
            "55096\n",
            "55097\n",
            "55098\n",
            "55099\n",
            "55100\n",
            "55101\n",
            "55102\n",
            "55103\n",
            "55104\n",
            "55105\n",
            "55106\n",
            "55107\n",
            "55108\n",
            "55109\n",
            "55110\n",
            "55111\n",
            "55112\n",
            "55113\n",
            "55114\n",
            "55115\n",
            "55116\n",
            "55117\n",
            "55118\n",
            "55119\n",
            "55120\n",
            "55121\n",
            "55122\n",
            "55123\n",
            "55124\n",
            "55125\n",
            "55126\n",
            "55127\n",
            "55128\n",
            "55129\n",
            "55130\n",
            "55131\n",
            "55132\n",
            "55133\n",
            "55134\n",
            "55135\n",
            "55136\n",
            "55137\n",
            "55138\n",
            "55139\n",
            "55140\n",
            "55141\n",
            "55142\n",
            "55143\n",
            "55144\n",
            "55145\n",
            "55146\n",
            "55147\n",
            "55148\n",
            "55149\n",
            "55150\n",
            "55151\n",
            "55152\n",
            "55153\n",
            "55154\n",
            "55155\n",
            "55156\n",
            "55157\n",
            "55158\n",
            "55159\n",
            "55160\n",
            "55161\n",
            "55162\n",
            "55163\n",
            "55164\n",
            "55165\n",
            "55166\n",
            "55167\n",
            "55168\n",
            "55169\n",
            "55170\n",
            "55171\n",
            "55172\n",
            "55173\n",
            "55174\n",
            "55175\n",
            "55176\n",
            "55177\n",
            "55178\n",
            "55179\n",
            "55180\n",
            "55181\n",
            "55182\n",
            "55183\n",
            "55184\n",
            "55185\n",
            "55186\n",
            "55187\n",
            "55188\n",
            "55189\n",
            "55190\n",
            "55191\n",
            "55192\n",
            "55193\n",
            "55194\n",
            "55195\n",
            "55196\n",
            "55197\n",
            "55198\n",
            "55199\n",
            "55200\n",
            "55201\n",
            "55202\n",
            "55203\n",
            "55204\n",
            "55205\n",
            "55206\n",
            "55207\n",
            "55208\n",
            "55209\n",
            "55210\n",
            "55211\n",
            "55212\n",
            "55213\n",
            "55214\n",
            "55215\n",
            "55216\n",
            "55217\n",
            "55218\n",
            "55219\n",
            "55220\n",
            "55221\n",
            "55222\n",
            "55223\n",
            "55224\n",
            "55225\n",
            "55226\n",
            "55227\n",
            "55228\n",
            "55229\n",
            "55230\n",
            "55231\n",
            "55232\n",
            "55233\n",
            "55234\n",
            "55235\n",
            "55236\n",
            "55237\n",
            "55238\n",
            "55239\n",
            "55240\n",
            "55241\n",
            "55242\n",
            "55243\n",
            "55244\n",
            "55245\n",
            "55246\n",
            "55247\n",
            "55248\n",
            "55249\n",
            "55250\n",
            "55251\n",
            "55252\n",
            "55253\n",
            "55254\n",
            "55255\n",
            "55256\n",
            "55257\n",
            "55258\n",
            "55259\n",
            "55260\n",
            "55261\n",
            "55262\n",
            "55263\n",
            "55264\n",
            "55265\n",
            "55266\n",
            "55267\n",
            "55268\n",
            "55269\n",
            "55270\n",
            "55271\n",
            "55272\n",
            "55273\n",
            "55274\n",
            "55275\n",
            "55276\n",
            "55277\n",
            "55278\n",
            "55279\n",
            "55280\n",
            "55281\n",
            "55282\n",
            "55283\n",
            "55284\n",
            "55285\n",
            "55286\n",
            "55287\n",
            "55288\n",
            "55289\n",
            "55290\n",
            "55291\n",
            "55292\n",
            "55293\n",
            "55294\n",
            "55295\n",
            "55296\n",
            "55297\n",
            "55298\n",
            "55299\n",
            "55300\n",
            "55301\n",
            "55302\n",
            "55303\n",
            "55304\n",
            "55305\n",
            "55306\n",
            "55307\n",
            "55308\n",
            "55309\n",
            "55310\n",
            "55311\n",
            "55312\n",
            "55313\n",
            "55314\n",
            "55315\n",
            "55316\n",
            "55317\n",
            "55318\n",
            "55319\n",
            "55320\n",
            "55321\n",
            "55322\n",
            "55323\n",
            "55324\n",
            "55325\n",
            "55326\n",
            "55327\n",
            "55328\n",
            "55329\n",
            "55330\n",
            "55331\n",
            "55332\n",
            "55333\n",
            "55334\n",
            "55335\n",
            "55336\n",
            "55337\n",
            "55338\n",
            "55339\n",
            "55340\n",
            "55341\n",
            "55342\n",
            "55343\n",
            "55344\n",
            "55345\n",
            "55346\n",
            "55347\n",
            "55348\n",
            "55349\n",
            "55350\n",
            "55351\n",
            "55352\n",
            "55353\n",
            "55354\n",
            "55355\n",
            "55356\n",
            "55357\n",
            "55358\n",
            "55359\n",
            "55360\n",
            "55361\n",
            "55362\n",
            "55363\n",
            "55364\n",
            "55365\n",
            "55366\n",
            "55367\n",
            "55368\n",
            "55369\n",
            "55370\n",
            "55371\n",
            "55372\n",
            "55373\n",
            "55374\n",
            "55375\n",
            "55376\n",
            "55377\n",
            "55378\n",
            "55379\n",
            "55380\n",
            "55381\n",
            "55382\n",
            "55383\n",
            "55384\n",
            "55385\n",
            "55386\n",
            "55387\n",
            "55388\n",
            "55389\n",
            "55390\n",
            "55391\n",
            "55392\n",
            "55393\n",
            "55394\n",
            "55395\n",
            "55396\n",
            "55397\n",
            "55398\n",
            "55399\n",
            "55400\n",
            "55401\n",
            "55402\n",
            "55403\n",
            "55404\n",
            "55405\n",
            "55406\n",
            "55407\n",
            "55408\n",
            "55409\n",
            "55410\n",
            "55411\n",
            "55412\n",
            "55413\n",
            "55414\n",
            "55415\n",
            "55416\n",
            "55417\n",
            "55418\n",
            "55419\n",
            "55420\n",
            "55421\n",
            "55422\n",
            "55423\n",
            "55424\n",
            "55425\n",
            "55426\n",
            "55427\n",
            "55428\n",
            "55429\n",
            "55430\n",
            "55431\n",
            "55432\n",
            "55433\n",
            "55434\n",
            "55435\n",
            "55436\n",
            "55437\n",
            "55438\n",
            "55439\n",
            "55440\n",
            "55441\n",
            "55442\n",
            "55443\n",
            "55444\n",
            "55445\n",
            "55446\n",
            "55447\n",
            "55448\n",
            "55449\n",
            "55450\n",
            "55451\n",
            "55452\n",
            "55453\n",
            "55454\n",
            "55455\n",
            "55456\n",
            "55457\n",
            "55458\n",
            "55459\n",
            "55460\n",
            "55461\n",
            "55462\n",
            "55463\n",
            "55464\n",
            "55465\n",
            "55466\n",
            "55467\n",
            "55468\n",
            "55469\n",
            "55470\n",
            "55471\n",
            "55472\n",
            "55473\n",
            "55474\n",
            "55475\n",
            "55476\n",
            "55477\n",
            "55478\n",
            "55479\n",
            "55480\n",
            "55481\n",
            "55482\n",
            "55483\n",
            "55484\n",
            "55485\n",
            "55486\n",
            "55487\n",
            "55488\n",
            "55489\n",
            "55490\n",
            "55491\n",
            "55492\n",
            "55493\n",
            "55494\n",
            "55495\n",
            "55496\n",
            "55497\n",
            "55498\n",
            "55499\n",
            "55500\n",
            "55501\n",
            "55502\n",
            "55503\n",
            "55504\n",
            "55505\n",
            "55506\n",
            "55507\n",
            "55508\n",
            "55509\n",
            "55510\n",
            "55511\n",
            "55512\n",
            "55513\n",
            "55514\n",
            "55515\n",
            "55516\n",
            "55517\n",
            "55518\n",
            "55519\n",
            "55520\n",
            "55521\n",
            "55522\n",
            "55523\n",
            "55524\n",
            "55525\n",
            "55526\n",
            "55527\n",
            "55528\n",
            "55529\n",
            "55530\n",
            "55531\n",
            "55532\n",
            "55533\n",
            "55534\n",
            "55535\n",
            "55536\n",
            "55537\n",
            "55538\n",
            "55539\n",
            "55540\n",
            "55541\n",
            "55542\n",
            "55543\n",
            "55544\n",
            "55545\n",
            "55546\n",
            "55547\n",
            "55548\n",
            "55549\n",
            "55550\n",
            "55551\n",
            "55552\n",
            "55553\n",
            "55554\n",
            "55555\n",
            "55556\n",
            "55557\n",
            "55558\n",
            "55559\n",
            "55560\n",
            "55561\n",
            "55562\n",
            "55563\n",
            "55564\n",
            "55565\n",
            "55566\n",
            "55567\n",
            "55568\n",
            "55569\n",
            "55570\n",
            "55571\n",
            "55572\n",
            "55573\n",
            "55574\n",
            "55575\n",
            "55576\n",
            "55577\n",
            "55578\n",
            "55579\n",
            "55580\n",
            "55581\n",
            "55582\n",
            "55583\n",
            "55584\n",
            "55585\n",
            "55586\n",
            "55587\n",
            "55588\n",
            "55589\n",
            "55590\n",
            "55591\n",
            "55592\n",
            "55593\n",
            "55594\n",
            "55595\n",
            "55596\n",
            "55597\n",
            "55598\n",
            "55599\n",
            "55600\n",
            "55601\n",
            "55602\n",
            "55603\n",
            "55604\n",
            "55605\n",
            "55606\n",
            "55607\n",
            "55608\n",
            "55609\n",
            "55610\n",
            "55611\n",
            "55612\n",
            "55613\n",
            "55614\n",
            "55615\n",
            "55616\n",
            "55617\n",
            "55618\n",
            "55619\n",
            "55620\n",
            "55621\n",
            "55622\n",
            "55623\n",
            "55624\n",
            "55625\n",
            "55626\n",
            "55627\n",
            "55628\n",
            "55629\n",
            "55630\n",
            "55631\n",
            "55632\n",
            "55633\n",
            "55634\n",
            "55635\n",
            "55636\n",
            "55637\n",
            "55638\n",
            "55639\n",
            "55640\n",
            "55641\n",
            "55642\n",
            "55643\n",
            "55644\n",
            "55645\n",
            "55646\n",
            "55647\n",
            "55648\n",
            "55649\n",
            "55650\n",
            "55651\n",
            "55652\n",
            "55653\n",
            "55654\n",
            "55655\n",
            "55656\n",
            "55657\n",
            "55658\n",
            "55659\n",
            "55660\n",
            "55661\n",
            "55662\n",
            "55663\n",
            "55664\n",
            "55665\n",
            "55666\n",
            "55667\n",
            "55668\n",
            "55669\n",
            "55670\n",
            "55671\n",
            "55672\n",
            "55673\n",
            "55674\n",
            "55675\n",
            "55676\n",
            "55677\n",
            "55678\n",
            "55679\n",
            "55680\n",
            "55681\n",
            "55682\n",
            "55683\n",
            "55684\n",
            "55685\n",
            "55686\n",
            "55687\n",
            "55688\n",
            "55689\n",
            "55690\n",
            "55691\n",
            "55692\n",
            "55693\n",
            "55694\n",
            "55695\n",
            "55696\n",
            "55697\n",
            "55698\n",
            "55699\n",
            "55700\n",
            "55701\n",
            "55702\n",
            "55703\n",
            "55704\n",
            "55705\n",
            "55706\n",
            "55707\n",
            "55708\n",
            "55709\n",
            "55710\n",
            "55711\n",
            "55712\n",
            "55713\n",
            "55714\n",
            "55715\n",
            "55716\n",
            "55717\n",
            "55718\n",
            "55719\n",
            "55720\n",
            "55721\n",
            "55722\n",
            "55723\n",
            "55724\n",
            "55725\n",
            "55726\n",
            "55727\n",
            "55728\n",
            "55729\n",
            "55730\n",
            "55731\n",
            "55732\n",
            "55733\n",
            "55734\n",
            "55735\n",
            "55736\n",
            "55737\n",
            "55738\n",
            "55739\n",
            "55740\n",
            "55741\n",
            "55742\n",
            "55743\n",
            "55744\n",
            "55745\n",
            "55746\n",
            "55747\n",
            "55748\n",
            "55749\n",
            "55750\n",
            "55751\n",
            "55752\n",
            "55753\n",
            "55754\n",
            "55755\n",
            "55756\n",
            "55757\n",
            "55758\n",
            "55759\n",
            "55760\n",
            "55761\n",
            "55762\n",
            "55763\n",
            "55764\n",
            "55765\n",
            "55766\n",
            "55767\n",
            "55768\n",
            "55769\n",
            "55770\n",
            "55771\n",
            "55772\n",
            "55773\n",
            "55774\n",
            "55775\n",
            "55776\n",
            "55777\n",
            "55778\n",
            "55779\n",
            "55780\n",
            "55781\n",
            "55782\n",
            "55783\n",
            "55784\n",
            "55785\n",
            "55786\n",
            "55787\n",
            "55788\n",
            "55789\n",
            "55790\n",
            "55791\n",
            "55792\n",
            "55793\n",
            "55794\n",
            "55795\n",
            "55796\n",
            "55797\n",
            "55798\n",
            "55799\n",
            "55800\n",
            "55801\n",
            "55802\n",
            "55803\n",
            "55804\n",
            "55805\n",
            "55806\n",
            "55807\n",
            "55808\n",
            "55809\n",
            "55810\n",
            "55811\n",
            "55812\n",
            "55813\n",
            "55814\n",
            "55815\n",
            "55816\n",
            "55817\n",
            "55818\n",
            "55819\n",
            "55820\n",
            "55821\n",
            "55822\n",
            "55823\n",
            "55824\n",
            "55825\n",
            "55826\n",
            "55827\n",
            "55828\n",
            "55829\n",
            "55830\n",
            "55831\n",
            "55832\n",
            "55833\n",
            "55834\n",
            "55835\n",
            "55836\n",
            "55837\n",
            "55838\n",
            "55839\n",
            "55840\n",
            "55841\n",
            "55842\n",
            "55843\n",
            "55844\n",
            "55845\n",
            "55846\n",
            "55847\n",
            "55848\n",
            "55849\n",
            "55850\n",
            "55851\n",
            "55852\n",
            "55853\n",
            "55854\n",
            "55855\n",
            "55856\n",
            "55857\n",
            "55858\n",
            "55859\n",
            "55860\n",
            "55861\n",
            "55862\n",
            "55863\n",
            "55864\n",
            "55865\n",
            "55866\n",
            "55867\n",
            "55868\n",
            "55869\n",
            "55870\n",
            "55871\n",
            "55872\n",
            "55873\n",
            "55874\n",
            "55875\n",
            "55876\n",
            "55877\n",
            "55878\n",
            "55879\n",
            "55880\n",
            "55881\n",
            "55882\n",
            "55883\n",
            "55884\n",
            "55885\n",
            "55886\n",
            "55887\n",
            "55888\n",
            "55889\n",
            "55890\n",
            "55891\n",
            "55892\n",
            "55893\n",
            "55894\n",
            "55895\n",
            "55896\n",
            "55897\n",
            "55898\n",
            "55899\n",
            "55900\n",
            "55901\n",
            "55902\n",
            "55903\n",
            "55904\n",
            "55905\n",
            "55906\n",
            "55907\n",
            "55908\n",
            "55909\n",
            "55910\n",
            "55911\n",
            "55912\n",
            "55913\n",
            "55914\n",
            "55915\n",
            "55916\n",
            "55917\n",
            "55918\n",
            "55919\n",
            "55920\n",
            "55921\n",
            "55922\n",
            "55923\n",
            "55924\n",
            "55925\n",
            "55926\n",
            "55927\n",
            "55928\n",
            "55929\n",
            "55930\n",
            "55931\n",
            "55932\n",
            "55933\n",
            "55934\n",
            "55935\n",
            "55936\n",
            "55937\n",
            "55938\n",
            "55939\n",
            "55940\n",
            "55941\n",
            "55942\n",
            "55943\n",
            "55944\n",
            "55945\n",
            "55946\n",
            "55947\n",
            "55948\n",
            "55949\n",
            "55950\n",
            "55951\n",
            "55952\n",
            "55953\n",
            "55954\n",
            "55955\n",
            "55956\n",
            "55957\n",
            "55958\n",
            "55959\n",
            "55960\n",
            "55961\n",
            "55962\n",
            "55963\n",
            "55964\n",
            "55965\n",
            "55966\n",
            "55967\n",
            "55968\n",
            "55969\n",
            "55970\n",
            "55971\n",
            "55972\n",
            "55973\n",
            "55974\n",
            "55975\n",
            "55976\n",
            "55977\n",
            "55978\n",
            "55979\n",
            "55980\n",
            "55981\n",
            "55982\n",
            "55983\n",
            "55984\n",
            "55985\n",
            "55986\n",
            "55987\n",
            "55988\n",
            "55989\n",
            "55990\n",
            "55991\n",
            "55992\n",
            "55993\n",
            "55994\n",
            "55995\n",
            "55996\n",
            "55997\n",
            "55998\n",
            "55999\n",
            "56000\n",
            "56001\n",
            "56002\n",
            "56003\n",
            "56004\n",
            "56005\n",
            "56006\n",
            "56007\n",
            "56008\n",
            "56009\n",
            "56010\n",
            "56011\n",
            "56012\n",
            "56013\n",
            "56014\n",
            "56015\n",
            "56016\n",
            "56017\n",
            "56018\n",
            "56019\n",
            "56020\n",
            "56021\n",
            "56022\n",
            "56023\n",
            "56024\n",
            "56025\n",
            "56026\n",
            "56027\n",
            "56028\n",
            "56029\n",
            "56030\n",
            "56031\n",
            "56032\n",
            "56033\n",
            "56034\n",
            "56035\n",
            "56036\n",
            "56037\n",
            "56038\n",
            "56039\n",
            "56040\n",
            "56041\n",
            "56042\n",
            "56043\n",
            "56044\n",
            "56045\n",
            "56046\n",
            "56047\n",
            "56048\n",
            "56049\n",
            "56050\n",
            "56051\n",
            "56052\n",
            "56053\n",
            "56054\n",
            "56055\n",
            "56056\n",
            "56057\n",
            "56058\n",
            "56059\n",
            "56060\n",
            "56061\n",
            "56062\n",
            "56063\n",
            "56064\n",
            "56065\n",
            "56066\n",
            "56067\n",
            "56068\n",
            "56069\n",
            "56070\n",
            "56071\n",
            "56072\n",
            "56073\n",
            "56074\n",
            "56075\n",
            "56076\n",
            "56077\n",
            "56078\n",
            "56079\n",
            "56080\n",
            "56081\n",
            "56082\n",
            "56083\n",
            "56084\n",
            "56085\n",
            "56086\n",
            "56087\n",
            "56088\n",
            "56089\n",
            "56090\n",
            "56091\n",
            "56092\n",
            "56093\n",
            "56094\n",
            "56095\n",
            "56096\n",
            "56097\n",
            "56098\n",
            "56099\n",
            "56100\n",
            "56101\n",
            "56102\n",
            "56103\n",
            "56104\n",
            "56105\n",
            "56106\n",
            "56107\n",
            "56108\n",
            "56109\n",
            "56110\n",
            "56111\n",
            "56112\n",
            "56113\n",
            "56114\n",
            "56115\n",
            "56116\n",
            "56117\n",
            "56118\n",
            "56119\n",
            "56120\n",
            "56121\n",
            "56122\n",
            "56123\n",
            "56124\n",
            "56125\n",
            "56126\n",
            "56127\n",
            "56128\n",
            "56129\n",
            "56130\n",
            "56131\n",
            "56132\n",
            "56133\n",
            "56134\n",
            "56135\n",
            "56136\n",
            "56137\n",
            "56138\n",
            "56139\n",
            "56140\n",
            "56141\n",
            "56142\n",
            "56143\n",
            "56144\n",
            "56145\n",
            "56146\n",
            "56147\n",
            "56148\n",
            "56149\n",
            "56150\n",
            "56151\n",
            "56152\n",
            "56153\n",
            "56154\n",
            "56155\n",
            "56156\n",
            "56157\n",
            "56158\n",
            "56159\n",
            "56160\n",
            "56161\n",
            "56162\n",
            "56163\n",
            "56164\n",
            "56165\n",
            "56166\n",
            "56167\n",
            "56168\n",
            "56169\n",
            "56170\n",
            "56171\n",
            "56172\n",
            "56173\n",
            "56174\n",
            "56175\n",
            "56176\n",
            "56177\n",
            "56178\n",
            "56179\n",
            "56180\n",
            "56181\n",
            "56182\n",
            "56183\n",
            "56184\n",
            "56185\n",
            "56186\n",
            "56187\n",
            "56188\n",
            "56189\n",
            "56190\n",
            "56191\n",
            "56192\n",
            "56193\n",
            "56194\n",
            "56195\n",
            "56196\n",
            "56197\n",
            "56198\n",
            "56199\n",
            "56200\n",
            "56201\n",
            "56202\n",
            "56203\n",
            "56204\n",
            "56205\n",
            "56206\n",
            "56207\n",
            "56208\n",
            "56209\n",
            "56210\n",
            "56211\n",
            "56212\n",
            "56213\n",
            "56214\n",
            "56215\n",
            "56216\n",
            "56217\n",
            "56218\n",
            "56219\n",
            "56220\n",
            "56221\n",
            "56222\n",
            "56223\n",
            "56224\n",
            "56225\n",
            "56226\n",
            "56227\n",
            "56228\n",
            "56229\n",
            "56230\n",
            "56231\n",
            "56232\n",
            "56233\n",
            "56234\n",
            "56235\n",
            "56236\n",
            "56237\n",
            "56238\n",
            "56239\n",
            "56240\n",
            "56241\n",
            "56242\n",
            "56243\n",
            "56244\n",
            "56245\n",
            "56246\n",
            "56247\n",
            "56248\n",
            "56249\n",
            "56250\n",
            "56251\n",
            "56252\n",
            "56253\n",
            "56254\n",
            "56255\n",
            "56256\n",
            "56257\n",
            "56258\n",
            "56259\n",
            "56260\n",
            "56261\n",
            "56262\n",
            "56263\n",
            "56264\n",
            "56265\n",
            "56266\n",
            "56267\n",
            "56268\n",
            "56269\n",
            "56270\n",
            "56271\n",
            "56272\n",
            "56273\n",
            "56274\n",
            "56275\n",
            "56276\n",
            "56277\n",
            "56278\n",
            "56279\n",
            "56280\n",
            "56281\n",
            "56282\n",
            "56283\n",
            "56284\n",
            "56285\n",
            "56286\n",
            "56287\n",
            "56288\n",
            "56289\n",
            "56290\n",
            "56291\n",
            "56292\n",
            "56293\n",
            "56294\n",
            "56295\n",
            "56296\n",
            "56297\n",
            "56298\n",
            "56299\n",
            "56300\n",
            "56301\n",
            "56302\n",
            "56303\n",
            "56304\n",
            "56305\n",
            "56306\n",
            "56307\n",
            "56308\n",
            "56309\n",
            "56310\n",
            "56311\n",
            "56312\n",
            "56313\n",
            "56314\n",
            "56315\n",
            "56316\n",
            "56317\n",
            "56318\n",
            "56319\n",
            "56320\n",
            "56321\n",
            "56322\n",
            "56323\n",
            "56324\n",
            "56325\n",
            "56326\n",
            "56327\n",
            "56328\n",
            "56329\n",
            "56330\n",
            "56331\n",
            "56332\n",
            "56333\n",
            "56334\n",
            "56335\n",
            "56336\n",
            "56337\n",
            "56338\n",
            "56339\n",
            "56340\n",
            "56341\n",
            "56342\n",
            "56343\n",
            "56344\n",
            "56345\n",
            "56346\n",
            "56347\n",
            "56348\n",
            "56349\n",
            "56350\n",
            "56351\n",
            "56352\n",
            "56353\n",
            "56354\n",
            "56355\n",
            "56356\n",
            "56357\n",
            "56358\n",
            "56359\n",
            "56360\n",
            "56361\n",
            "56362\n",
            "56363\n",
            "56364\n",
            "56365\n",
            "56366\n",
            "56367\n",
            "56368\n",
            "56369\n",
            "56370\n",
            "56371\n",
            "56372\n",
            "56373\n",
            "56374\n",
            "56375\n",
            "56376\n",
            "56377\n",
            "56378\n",
            "56379\n",
            "56380\n",
            "56381\n",
            "56382\n",
            "56383\n",
            "56384\n",
            "56385\n",
            "56386\n",
            "56387\n",
            "56388\n",
            "56389\n",
            "56390\n",
            "56391\n",
            "56392\n",
            "56393\n",
            "56394\n",
            "56395\n",
            "56396\n",
            "56397\n",
            "56398\n",
            "56399\n",
            "56400\n",
            "56401\n",
            "56402\n",
            "56403\n",
            "56404\n",
            "56405\n",
            "56406\n",
            "56407\n",
            "56408\n",
            "56409\n",
            "56410\n",
            "56411\n",
            "56412\n",
            "56413\n",
            "56414\n",
            "56415\n",
            "56416\n",
            "56417\n",
            "56418\n",
            "56419\n",
            "56420\n",
            "56421\n",
            "56422\n",
            "56423\n",
            "56424\n",
            "56425\n",
            "56426\n",
            "56427\n",
            "56428\n",
            "56429\n",
            "56430\n",
            "56431\n",
            "56432\n",
            "56433\n",
            "56434\n",
            "56435\n",
            "56436\n",
            "56437\n",
            "56438\n",
            "56439\n",
            "56440\n",
            "56441\n",
            "56442\n",
            "56443\n",
            "56444\n",
            "56445\n",
            "56446\n",
            "56447\n",
            "56448\n",
            "56449\n",
            "56450\n",
            "56451\n",
            "56452\n",
            "56453\n",
            "56454\n",
            "56455\n",
            "56456\n",
            "56457\n",
            "56458\n",
            "56459\n",
            "56460\n",
            "56461\n",
            "56462\n",
            "56463\n",
            "56464\n",
            "56465\n",
            "56466\n",
            "56467\n",
            "56468\n",
            "56469\n",
            "56470\n",
            "56471\n",
            "56472\n",
            "56473\n",
            "56474\n",
            "56475\n",
            "56476\n",
            "56477\n",
            "56478\n",
            "56479\n",
            "56480\n",
            "56481\n",
            "56482\n",
            "56483\n",
            "56484\n",
            "56485\n",
            "56486\n",
            "56487\n",
            "56488\n",
            "56489\n",
            "56490\n",
            "56491\n",
            "56492\n",
            "56493\n",
            "56494\n",
            "56495\n",
            "56496\n",
            "56497\n",
            "56498\n",
            "56499\n",
            "56500\n",
            "56501\n",
            "56502\n",
            "56503\n",
            "56504\n",
            "56505\n",
            "56506\n",
            "56507\n",
            "56508\n",
            "56509\n",
            "56510\n",
            "56511\n",
            "56512\n",
            "56513\n",
            "56514\n",
            "56515\n",
            "56516\n",
            "56517\n",
            "56518\n",
            "56519\n",
            "56520\n",
            "56521\n",
            "56522\n",
            "56523\n",
            "56524\n",
            "56525\n",
            "56526\n",
            "56527\n",
            "56528\n",
            "56529\n",
            "56530\n",
            "56531\n",
            "56532\n",
            "56533\n",
            "56534\n",
            "56535\n",
            "56536\n",
            "56537\n",
            "56538\n",
            "56539\n",
            "56540\n",
            "56541\n",
            "56542\n",
            "56543\n",
            "56544\n",
            "56545\n",
            "56546\n",
            "56547\n",
            "56548\n",
            "56549\n",
            "56550\n",
            "56551\n",
            "56552\n",
            "56553\n",
            "56554\n",
            "56555\n",
            "56556\n",
            "56557\n",
            "56558\n",
            "56559\n",
            "56560\n",
            "56561\n",
            "56562\n",
            "56563\n",
            "56564\n",
            "56565\n",
            "56566\n",
            "56567\n",
            "56568\n",
            "56569\n",
            "56570\n",
            "56571\n",
            "56572\n",
            "56573\n",
            "56574\n",
            "56575\n",
            "56576\n",
            "56577\n",
            "56578\n",
            "56579\n",
            "56580\n",
            "56581\n",
            "56582\n",
            "56583\n",
            "56584\n",
            "56585\n",
            "56586\n",
            "56587\n",
            "56588\n",
            "56589\n",
            "56590\n",
            "56591\n",
            "56592\n",
            "56593\n",
            "56594\n",
            "56595\n",
            "56596\n",
            "56597\n",
            "56598\n",
            "56599\n",
            "56600\n",
            "56601\n",
            "56602\n",
            "56603\n",
            "56604\n",
            "56605\n",
            "56606\n",
            "56607\n",
            "56608\n",
            "56609\n",
            "56610\n",
            "56611\n",
            "56612\n",
            "56613\n",
            "56614\n",
            "56615\n",
            "56616\n",
            "56617\n",
            "56618\n",
            "56619\n",
            "56620\n",
            "56621\n",
            "56622\n",
            "56623\n",
            "56624\n",
            "56625\n",
            "56626\n",
            "56627\n",
            "56628\n",
            "56629\n",
            "56630\n",
            "56631\n",
            "56632\n",
            "56633\n",
            "56634\n",
            "56635\n",
            "56636\n",
            "56637\n",
            "56638\n",
            "56639\n",
            "56640\n",
            "56641\n",
            "56642\n",
            "56643\n",
            "56644\n",
            "56645\n",
            "56646\n",
            "56647\n",
            "56648\n",
            "56649\n",
            "56650\n",
            "56651\n",
            "56652\n",
            "56653\n",
            "56654\n",
            "56655\n",
            "56656\n",
            "56657\n",
            "56658\n",
            "56659\n",
            "56660\n",
            "56661\n",
            "56662\n",
            "56663\n",
            "56664\n",
            "56665\n",
            "56666\n",
            "56667\n",
            "56668\n",
            "56669\n",
            "56670\n",
            "56671\n",
            "56672\n",
            "56673\n",
            "56674\n",
            "56675\n",
            "56676\n",
            "56677\n",
            "56678\n",
            "56679\n",
            "56680\n",
            "56681\n",
            "56682\n",
            "56683\n",
            "56684\n",
            "56685\n",
            "56686\n",
            "56687\n",
            "56688\n",
            "56689\n",
            "56690\n",
            "56691\n",
            "56692\n",
            "56693\n",
            "56694\n",
            "56695\n",
            "56696\n",
            "56697\n",
            "56698\n",
            "56699\n",
            "56700\n",
            "56701\n",
            "56702\n",
            "56703\n",
            "56704\n",
            "56705\n",
            "56706\n",
            "56707\n",
            "56708\n",
            "56709\n",
            "56710\n",
            "56711\n",
            "56712\n",
            "56713\n",
            "56714\n",
            "56715\n",
            "56716\n",
            "56717\n",
            "56718\n",
            "56719\n",
            "56720\n",
            "56721\n",
            "56722\n",
            "56723\n",
            "56724\n",
            "56725\n",
            "56726\n",
            "56727\n",
            "56728\n",
            "56729\n",
            "56730\n",
            "56731\n",
            "56732\n",
            "56733\n",
            "56734\n",
            "56735\n",
            "56736\n",
            "56737\n",
            "56738\n",
            "56739\n",
            "56740\n",
            "56741\n",
            "56742\n",
            "56743\n",
            "56744\n",
            "56745\n",
            "56746\n",
            "56747\n",
            "56748\n",
            "56749\n",
            "56750\n",
            "56751\n",
            "56752\n",
            "56753\n",
            "56754\n",
            "56755\n",
            "56756\n",
            "56757\n",
            "56758\n",
            "56759\n",
            "56760\n",
            "56761\n",
            "56762\n",
            "56763\n",
            "56764\n",
            "56765\n",
            "56766\n",
            "56767\n",
            "56768\n",
            "56769\n",
            "56770\n",
            "56771\n",
            "56772\n",
            "56773\n",
            "56774\n",
            "56775\n",
            "56776\n",
            "56777\n",
            "56778\n",
            "56779\n",
            "56780\n",
            "56781\n",
            "56782\n",
            "56783\n",
            "56784\n",
            "56785\n",
            "56786\n",
            "56787\n",
            "56788\n",
            "56789\n",
            "56790\n",
            "56791\n",
            "56792\n",
            "56793\n",
            "56794\n",
            "56795\n",
            "56796\n",
            "56797\n",
            "56798\n",
            "56799\n",
            "56800\n",
            "56801\n",
            "56802\n",
            "56803\n",
            "56804\n",
            "56805\n",
            "56806\n",
            "56807\n",
            "56808\n",
            "56809\n",
            "56810\n",
            "56811\n",
            "56812\n",
            "56813\n",
            "56814\n",
            "56815\n",
            "56816\n",
            "56817\n",
            "56818\n",
            "56819\n",
            "56820\n",
            "56821\n",
            "56822\n",
            "56823\n",
            "56824\n",
            "56825\n",
            "56826\n",
            "56827\n",
            "56828\n",
            "56829\n",
            "56830\n",
            "56831\n",
            "56832\n",
            "56833\n",
            "56834\n",
            "56835\n",
            "56836\n",
            "56837\n",
            "56838\n",
            "56839\n",
            "56840\n",
            "56841\n",
            "56842\n",
            "56843\n",
            "56844\n",
            "56845\n",
            "56846\n",
            "56847\n",
            "56848\n",
            "56849\n",
            "56850\n",
            "56851\n",
            "56852\n",
            "56853\n",
            "56854\n",
            "56855\n",
            "56856\n",
            "56857\n",
            "56858\n",
            "56859\n",
            "56860\n",
            "56861\n",
            "56862\n",
            "56863\n",
            "56864\n",
            "56865\n",
            "56866\n",
            "56867\n",
            "56868\n",
            "56869\n",
            "56870\n",
            "56871\n",
            "56872\n",
            "56873\n",
            "56874\n",
            "56875\n",
            "56876\n",
            "56877\n",
            "56878\n",
            "56879\n",
            "56880\n",
            "56881\n",
            "56882\n",
            "56883\n",
            "56884\n",
            "56885\n",
            "56886\n",
            "56887\n",
            "56888\n",
            "56889\n",
            "56890\n",
            "56891\n",
            "56892\n",
            "56893\n",
            "56894\n",
            "56895\n",
            "56896\n",
            "56897\n",
            "56898\n",
            "56899\n",
            "56900\n",
            "56901\n",
            "56902\n",
            "56903\n",
            "56904\n",
            "56905\n",
            "56906\n",
            "56907\n",
            "56908\n",
            "56909\n",
            "56910\n",
            "56911\n",
            "56912\n",
            "56913\n",
            "56914\n",
            "56915\n",
            "56916\n",
            "56917\n",
            "56918\n",
            "56919\n",
            "56920\n",
            "56921\n",
            "56922\n",
            "56923\n",
            "56924\n",
            "56925\n",
            "56926\n",
            "56927\n",
            "56928\n",
            "56929\n",
            "56930\n",
            "56931\n",
            "56932\n",
            "56933\n",
            "56934\n",
            "56935\n",
            "56936\n",
            "56937\n",
            "56938\n",
            "56939\n",
            "56940\n",
            "56941\n",
            "56942\n",
            "56943\n",
            "56944\n",
            "56945\n",
            "56946\n",
            "56947\n",
            "56948\n",
            "56949\n",
            "56950\n",
            "56951\n",
            "56952\n",
            "56953\n",
            "56954\n",
            "56955\n",
            "56956\n",
            "56957\n",
            "56958\n",
            "56959\n",
            "56960\n",
            "56961\n",
            "56962\n",
            "56963\n",
            "56964\n",
            "56965\n",
            "56966\n",
            "56967\n",
            "56968\n",
            "56969\n",
            "56970\n",
            "56971\n",
            "56972\n",
            "56973\n",
            "56974\n",
            "56975\n",
            "56976\n",
            "56977\n",
            "56978\n",
            "56979\n",
            "56980\n",
            "56981\n",
            "56982\n",
            "56983\n",
            "56984\n",
            "56985\n",
            "56986\n",
            "56987\n",
            "56988\n",
            "56989\n",
            "56990\n",
            "56991\n",
            "56992\n",
            "56993\n",
            "56994\n",
            "56995\n",
            "56996\n",
            "56997\n",
            "56998\n",
            "56999\n",
            "57000\n",
            "57001\n",
            "57002\n",
            "57003\n",
            "57004\n",
            "57005\n",
            "57006\n",
            "57007\n",
            "57008\n",
            "57009\n",
            "57010\n",
            "57011\n",
            "57012\n",
            "57013\n",
            "57014\n",
            "57015\n",
            "57016\n",
            "57017\n",
            "57018\n",
            "57019\n",
            "57020\n",
            "57021\n",
            "57022\n",
            "57023\n",
            "57024\n",
            "57025\n",
            "57026\n",
            "57027\n",
            "57028\n",
            "57029\n",
            "57030\n",
            "57031\n",
            "57032\n",
            "57033\n",
            "57034\n",
            "57035\n",
            "57036\n",
            "57037\n",
            "57038\n",
            "57039\n",
            "57040\n",
            "57041\n",
            "57042\n",
            "57043\n",
            "57044\n",
            "57045\n",
            "57046\n",
            "57047\n",
            "57048\n",
            "57049\n",
            "57050\n",
            "57051\n",
            "57052\n",
            "57053\n",
            "57054\n",
            "57055\n",
            "57056\n",
            "57057\n",
            "57058\n",
            "57059\n",
            "57060\n",
            "57061\n",
            "57062\n",
            "57063\n",
            "57064\n",
            "57065\n",
            "57066\n",
            "57067\n",
            "57068\n",
            "57069\n",
            "57070\n",
            "57071\n",
            "57072\n",
            "57073\n",
            "57074\n",
            "57075\n",
            "57076\n",
            "57077\n",
            "57078\n",
            "57079\n",
            "57080\n",
            "57081\n",
            "57082\n",
            "57083\n",
            "57084\n",
            "57085\n",
            "57086\n",
            "57087\n",
            "57088\n",
            "57089\n",
            "57090\n",
            "57091\n",
            "57092\n",
            "57093\n",
            "57094\n",
            "57095\n",
            "57096\n",
            "57097\n",
            "57098\n",
            "57099\n",
            "57100\n",
            "57101\n",
            "57102\n",
            "57103\n",
            "57104\n",
            "57105\n",
            "57106\n",
            "57107\n",
            "57108\n",
            "57109\n",
            "57110\n",
            "57111\n",
            "57112\n",
            "57113\n",
            "57114\n",
            "57115\n",
            "57116\n",
            "57117\n",
            "57118\n",
            "57119\n",
            "57120\n",
            "57121\n",
            "57122\n",
            "57123\n",
            "57124\n",
            "57125\n",
            "57126\n",
            "57127\n",
            "57128\n",
            "57129\n",
            "57130\n",
            "57131\n",
            "57132\n",
            "57133\n",
            "57134\n",
            "57135\n",
            "57136\n",
            "57137\n",
            "57138\n",
            "57139\n",
            "57140\n",
            "57141\n",
            "57142\n",
            "57143\n",
            "57144\n",
            "57145\n",
            "57146\n",
            "57147\n",
            "57148\n",
            "57149\n",
            "57150\n",
            "57151\n",
            "57152\n",
            "57153\n",
            "57154\n",
            "57155\n",
            "57156\n",
            "57157\n",
            "57158\n",
            "57159\n",
            "57160\n",
            "57161\n",
            "57162\n",
            "57163\n",
            "57164\n",
            "57165\n",
            "57166\n",
            "57167\n",
            "57168\n",
            "57169\n",
            "57170\n",
            "57171\n",
            "57172\n",
            "57173\n",
            "57174\n",
            "57175\n",
            "57176\n",
            "57177\n",
            "57178\n",
            "57179\n",
            "57180\n",
            "57181\n",
            "57182\n",
            "57183\n",
            "57184\n",
            "57185\n",
            "57186\n",
            "57187\n",
            "57188\n",
            "57189\n",
            "57190\n",
            "57191\n",
            "57192\n",
            "57193\n",
            "57194\n",
            "57195\n",
            "57196\n",
            "57197\n",
            "57198\n",
            "57199\n",
            "57200\n",
            "57201\n",
            "57202\n",
            "57203\n",
            "57204\n",
            "57205\n",
            "57206\n",
            "57207\n",
            "57208\n",
            "57209\n",
            "57210\n",
            "57211\n",
            "57212\n",
            "57213\n",
            "57214\n",
            "57215\n",
            "57216\n",
            "57217\n",
            "57218\n",
            "57219\n",
            "57220\n",
            "57221\n",
            "57222\n",
            "57223\n",
            "57224\n",
            "57225\n",
            "57226\n",
            "57227\n",
            "57228\n",
            "57229\n",
            "57230\n",
            "57231\n",
            "57232\n",
            "57233\n",
            "57234\n",
            "57235\n",
            "57236\n",
            "57237\n",
            "57238\n",
            "57239\n",
            "57240\n",
            "57241\n",
            "57242\n",
            "57243\n",
            "57244\n",
            "57245\n",
            "57246\n",
            "57247\n",
            "57248\n",
            "57249\n",
            "57250\n",
            "57251\n",
            "57252\n",
            "57253\n",
            "57254\n",
            "57255\n",
            "57256\n",
            "57257\n",
            "57258\n",
            "57259\n",
            "57260\n",
            "57261\n",
            "57262\n",
            "57263\n",
            "57264\n",
            "57265\n",
            "57266\n",
            "57267\n",
            "57268\n",
            "57269\n",
            "57270\n",
            "57271\n",
            "57272\n",
            "57273\n",
            "57274\n",
            "57275\n",
            "57276\n",
            "57277\n",
            "57278\n",
            "57279\n",
            "57280\n",
            "57281\n",
            "57282\n",
            "57283\n",
            "57284\n",
            "57285\n",
            "57286\n",
            "57287\n",
            "57288\n",
            "57289\n",
            "57290\n",
            "57291\n",
            "57292\n",
            "57293\n",
            "57294\n",
            "57295\n",
            "57296\n",
            "57297\n",
            "57298\n",
            "57299\n",
            "57300\n",
            "57301\n",
            "57302\n",
            "57303\n",
            "57304\n",
            "57305\n",
            "57306\n",
            "57307\n",
            "57308\n",
            "57309\n",
            "57310\n",
            "57311\n",
            "57312\n",
            "57313\n",
            "57314\n",
            "57315\n",
            "57316\n",
            "57317\n",
            "57318\n",
            "57319\n",
            "57320\n",
            "57321\n",
            "57322\n",
            "57323\n",
            "57324\n",
            "57325\n",
            "57326\n",
            "57327\n",
            "57328\n",
            "57329\n",
            "57330\n",
            "57331\n",
            "57332\n",
            "57333\n",
            "57334\n",
            "57335\n",
            "57336\n",
            "57337\n",
            "57338\n",
            "57339\n",
            "57340\n",
            "57341\n",
            "57342\n",
            "57343\n",
            "57344\n",
            "57345\n",
            "57346\n",
            "57347\n",
            "57348\n",
            "57349\n",
            "57350\n",
            "57351\n",
            "57352\n",
            "57353\n",
            "57354\n",
            "57355\n",
            "57356\n",
            "57357\n",
            "57358\n",
            "57359\n",
            "57360\n",
            "57361\n",
            "57362\n",
            "57363\n",
            "57364\n",
            "57365\n",
            "57366\n",
            "57367\n",
            "57368\n",
            "57369\n",
            "57370\n",
            "57371\n",
            "57372\n",
            "57373\n",
            "57374\n",
            "57375\n",
            "57376\n",
            "57377\n",
            "57378\n",
            "57379\n",
            "57380\n",
            "57381\n",
            "57382\n",
            "57383\n",
            "57384\n",
            "57385\n",
            "57386\n",
            "57387\n",
            "57388\n",
            "57389\n",
            "57390\n",
            "57391\n",
            "57392\n",
            "57393\n",
            "57394\n",
            "57395\n",
            "57396\n",
            "57397\n",
            "57398\n",
            "57399\n",
            "57400\n",
            "57401\n",
            "57402\n",
            "57403\n",
            "57404\n",
            "57405\n",
            "57406\n",
            "57407\n",
            "57408\n",
            "57409\n",
            "57410\n",
            "57411\n",
            "57412\n",
            "57413\n",
            "57414\n",
            "57415\n",
            "57416\n",
            "57417\n",
            "57418\n",
            "57419\n",
            "57420\n",
            "57421\n",
            "57422\n",
            "57423\n",
            "57424\n",
            "57425\n",
            "57426\n",
            "57427\n",
            "57428\n",
            "57429\n",
            "57430\n",
            "57431\n",
            "57432\n",
            "57433\n",
            "57434\n",
            "57435\n",
            "57436\n",
            "57437\n",
            "57438\n",
            "57439\n",
            "57440\n",
            "57441\n",
            "57442\n",
            "57443\n",
            "57444\n",
            "57445\n",
            "57446\n",
            "57447\n",
            "57448\n",
            "57449\n",
            "57450\n",
            "57451\n",
            "57452\n",
            "57453\n",
            "57454\n",
            "57455\n",
            "57456\n",
            "57457\n",
            "57458\n",
            "57459\n",
            "57460\n",
            "57461\n",
            "57462\n",
            "57463\n",
            "57464\n",
            "57465\n",
            "57466\n",
            "57467\n",
            "57468\n",
            "57469\n",
            "57470\n",
            "57471\n",
            "57472\n",
            "57473\n",
            "57474\n",
            "57475\n",
            "57476\n",
            "57477\n",
            "57478\n",
            "57479\n",
            "57480\n",
            "57481\n",
            "57482\n",
            "57483\n",
            "57484\n",
            "57485\n",
            "57486\n",
            "57487\n",
            "57488\n",
            "57489\n",
            "57490\n",
            "57491\n",
            "57492\n",
            "57493\n",
            "57494\n",
            "57495\n",
            "57496\n",
            "57497\n",
            "57498\n",
            "57499\n",
            "57500\n",
            "57501\n",
            "57502\n",
            "57503\n",
            "57504\n",
            "57505\n",
            "57506\n",
            "57507\n",
            "57508\n",
            "57509\n",
            "57510\n",
            "57511\n",
            "57512\n",
            "57513\n",
            "57514\n",
            "57515\n",
            "57516\n",
            "57517\n",
            "57518\n",
            "57519\n",
            "57520\n",
            "57521\n",
            "57522\n",
            "57523\n",
            "57524\n",
            "57525\n",
            "57526\n",
            "57527\n",
            "57528\n",
            "57529\n",
            "57530\n",
            "57531\n",
            "57532\n",
            "57533\n",
            "57534\n",
            "57535\n",
            "57536\n",
            "57537\n",
            "57538\n",
            "57539\n",
            "57540\n",
            "57541\n",
            "57542\n",
            "57543\n",
            "57544\n",
            "57545\n",
            "57546\n",
            "57547\n",
            "57548\n",
            "57549\n",
            "57550\n",
            "57551\n",
            "57552\n",
            "57553\n",
            "57554\n",
            "57555\n",
            "57556\n",
            "57557\n",
            "57558\n",
            "57559\n",
            "57560\n",
            "57561\n",
            "57562\n",
            "57563\n",
            "57564\n",
            "57565\n",
            "57566\n",
            "57567\n",
            "57568\n",
            "57569\n",
            "57570\n",
            "57571\n",
            "57572\n",
            "57573\n",
            "57574\n",
            "57575\n",
            "57576\n",
            "57577\n",
            "57578\n",
            "57579\n",
            "57580\n",
            "57581\n",
            "57582\n",
            "57583\n",
            "57584\n",
            "57585\n",
            "57586\n",
            "57587\n",
            "57588\n",
            "57589\n",
            "57590\n",
            "57591\n",
            "57592\n",
            "57593\n",
            "57594\n",
            "57595\n",
            "57596\n",
            "57597\n",
            "57598\n",
            "57599\n",
            "57600\n",
            "57601\n",
            "57602\n",
            "57603\n",
            "57604\n",
            "57605\n",
            "57606\n",
            "57607\n",
            "57608\n",
            "57609\n",
            "57610\n",
            "57611\n",
            "57612\n",
            "57613\n",
            "57614\n",
            "57615\n",
            "57616\n",
            "57617\n",
            "57618\n",
            "57619\n",
            "57620\n",
            "57621\n",
            "57622\n",
            "57623\n",
            "57624\n",
            "57625\n",
            "57626\n",
            "57627\n",
            "57628\n",
            "57629\n",
            "57630\n",
            "57631\n",
            "57632\n",
            "57633\n",
            "57634\n",
            "57635\n",
            "57636\n",
            "57637\n",
            "57638\n",
            "57639\n",
            "57640\n",
            "57641\n",
            "57642\n",
            "57643\n",
            "57644\n",
            "57645\n",
            "57646\n",
            "57647\n",
            "57648\n",
            "57649\n",
            "57650\n",
            "57651\n",
            "57652\n",
            "57653\n",
            "57654\n",
            "57655\n",
            "57656\n",
            "57657\n",
            "57658\n",
            "57659\n",
            "57660\n",
            "57661\n",
            "57662\n",
            "57663\n",
            "57664\n",
            "57665\n",
            "57666\n",
            "57667\n",
            "57668\n",
            "57669\n",
            "57670\n",
            "57671\n",
            "57672\n",
            "57673\n",
            "57674\n",
            "57675\n",
            "57676\n",
            "57677\n",
            "57678\n",
            "57679\n",
            "57680\n",
            "57681\n",
            "57682\n",
            "57683\n",
            "57684\n",
            "57685\n",
            "57686\n",
            "57687\n",
            "57688\n",
            "57689\n",
            "57690\n",
            "57691\n",
            "57692\n",
            "57693\n",
            "57694\n",
            "57695\n",
            "57696\n",
            "57697\n",
            "57698\n",
            "57699\n",
            "57700\n",
            "57701\n",
            "57702\n",
            "57703\n",
            "57704\n",
            "57705\n",
            "57706\n",
            "57707\n",
            "57708\n",
            "57709\n",
            "57710\n",
            "57711\n",
            "57712\n",
            "57713\n",
            "57714\n",
            "57715\n",
            "57716\n",
            "57717\n",
            "57718\n",
            "57719\n",
            "57720\n",
            "57721\n",
            "57722\n",
            "57723\n",
            "57724\n",
            "57725\n",
            "57726\n",
            "57727\n",
            "57728\n",
            "57729\n",
            "57730\n",
            "57731\n",
            "57732\n",
            "57733\n",
            "57734\n",
            "57735\n",
            "57736\n",
            "57737\n",
            "57738\n",
            "57739\n",
            "57740\n",
            "57741\n",
            "57742\n",
            "57743\n",
            "57744\n",
            "57745\n",
            "57746\n",
            "57747\n",
            "57748\n",
            "57749\n",
            "57750\n",
            "57751\n",
            "57752\n",
            "57753\n",
            "57754\n",
            "57755\n",
            "57756\n",
            "57757\n",
            "57758\n",
            "57759\n",
            "57760\n",
            "57761\n",
            "57762\n",
            "57763\n",
            "57764\n",
            "57765\n",
            "57766\n",
            "57767\n",
            "57768\n",
            "57769\n",
            "57770\n",
            "57771\n",
            "57772\n",
            "57773\n",
            "57774\n",
            "57775\n",
            "57776\n",
            "57777\n",
            "57778\n",
            "57779\n",
            "57780\n",
            "57781\n",
            "57782\n",
            "57783\n",
            "57784\n",
            "57785\n",
            "57786\n",
            "57787\n",
            "57788\n",
            "57789\n",
            "57790\n",
            "57791\n",
            "57792\n",
            "57793\n",
            "57794\n",
            "57795\n",
            "57796\n",
            "57797\n",
            "57798\n",
            "57799\n",
            "57800\n",
            "57801\n",
            "57802\n",
            "57803\n",
            "57804\n",
            "57805\n",
            "57806\n",
            "57807\n",
            "57808\n",
            "57809\n",
            "57810\n",
            "57811\n",
            "57812\n",
            "57813\n",
            "57814\n",
            "57815\n",
            "57816\n",
            "57817\n",
            "57818\n",
            "57819\n",
            "57820\n",
            "57821\n",
            "57822\n",
            "57823\n",
            "57824\n",
            "57825\n",
            "57826\n",
            "57827\n",
            "57828\n",
            "57829\n",
            "57830\n",
            "57831\n",
            "57832\n",
            "57833\n",
            "57834\n",
            "57835\n",
            "57836\n",
            "57837\n",
            "57838\n",
            "57839\n",
            "57840\n",
            "57841\n",
            "57842\n",
            "57843\n",
            "57844\n",
            "57845\n",
            "57846\n",
            "57847\n",
            "57848\n",
            "57849\n",
            "57850\n",
            "57851\n",
            "57852\n",
            "57853\n",
            "57854\n",
            "57855\n",
            "57856\n",
            "57857\n",
            "57858\n",
            "57859\n",
            "57860\n",
            "57861\n",
            "57862\n",
            "57863\n",
            "57864\n",
            "57865\n",
            "57866\n",
            "57867\n",
            "57868\n",
            "57869\n",
            "57870\n",
            "57871\n",
            "57872\n",
            "57873\n",
            "57874\n",
            "57875\n",
            "57876\n",
            "57877\n",
            "57878\n",
            "57879\n",
            "57880\n",
            "57881\n",
            "57882\n",
            "57883\n",
            "57884\n",
            "57885\n",
            "57886\n",
            "57887\n",
            "57888\n",
            "57889\n",
            "57890\n",
            "57891\n",
            "57892\n",
            "57893\n",
            "57894\n",
            "57895\n",
            "57896\n",
            "57897\n",
            "57898\n",
            "57899\n",
            "57900\n",
            "57901\n",
            "57902\n",
            "57903\n",
            "57904\n",
            "57905\n",
            "57906\n",
            "57907\n",
            "57908\n",
            "57909\n",
            "57910\n",
            "57911\n",
            "57912\n",
            "57913\n",
            "57914\n",
            "57915\n",
            "57916\n",
            "57917\n",
            "57918\n",
            "57919\n",
            "57920\n",
            "57921\n",
            "57922\n",
            "57923\n",
            "57924\n",
            "57925\n",
            "57926\n",
            "57927\n",
            "57928\n",
            "57929\n",
            "57930\n",
            "57931\n",
            "57932\n",
            "57933\n",
            "57934\n",
            "57935\n",
            "57936\n",
            "57937\n",
            "57938\n",
            "57939\n",
            "57940\n",
            "57941\n",
            "57942\n",
            "57943\n",
            "57944\n",
            "57945\n",
            "57946\n",
            "57947\n",
            "57948\n",
            "57949\n",
            "57950\n",
            "57951\n",
            "57952\n",
            "57953\n",
            "57954\n",
            "57955\n",
            "57956\n",
            "57957\n",
            "57958\n",
            "57959\n",
            "57960\n",
            "57961\n",
            "57962\n",
            "57963\n",
            "57964\n",
            "57965\n",
            "57966\n",
            "57967\n",
            "57968\n",
            "57969\n",
            "57970\n",
            "57971\n",
            "57972\n",
            "57973\n",
            "57974\n",
            "57975\n",
            "57976\n",
            "57977\n",
            "57978\n",
            "57979\n",
            "57980\n",
            "57981\n",
            "57982\n",
            "57983\n",
            "57984\n",
            "57985\n",
            "57986\n",
            "57987\n",
            "57988\n",
            "57989\n",
            "57990\n",
            "57991\n",
            "57992\n",
            "57993\n",
            "57994\n",
            "57995\n",
            "57996\n",
            "57997\n",
            "57998\n",
            "57999\n",
            "58000\n",
            "58001\n",
            "58002\n",
            "58003\n",
            "58004\n",
            "58005\n",
            "58006\n",
            "58007\n",
            "58008\n",
            "58009\n",
            "58010\n",
            "58011\n",
            "58012\n",
            "58013\n",
            "58014\n",
            "58015\n",
            "58016\n",
            "58017\n",
            "58018\n",
            "58019\n",
            "58020\n",
            "58021\n",
            "58022\n",
            "58023\n",
            "58024\n",
            "58025\n",
            "58026\n",
            "58027\n",
            "58028\n",
            "58029\n",
            "58030\n",
            "58031\n",
            "58032\n",
            "58033\n",
            "58034\n",
            "58035\n",
            "58036\n",
            "58037\n",
            "58038\n",
            "58039\n",
            "58040\n",
            "58041\n",
            "58042\n",
            "58043\n",
            "58044\n",
            "58045\n",
            "58046\n",
            "58047\n",
            "58048\n",
            "58049\n",
            "58050\n",
            "58051\n",
            "58052\n",
            "58053\n",
            "58054\n",
            "58055\n",
            "58056\n",
            "58057\n",
            "58058\n",
            "58059\n",
            "58060\n",
            "58061\n",
            "58062\n",
            "58063\n",
            "58064\n",
            "58065\n",
            "58066\n",
            "58067\n",
            "58068\n",
            "58069\n",
            "58070\n",
            "58071\n",
            "58072\n",
            "58073\n",
            "58074\n",
            "58075\n",
            "58076\n",
            "58077\n",
            "58078\n",
            "58079\n",
            "58080\n",
            "58081\n",
            "58082\n",
            "58083\n",
            "58084\n",
            "58085\n",
            "58086\n",
            "58087\n",
            "58088\n",
            "58089\n",
            "58090\n",
            "58091\n",
            "58092\n",
            "58093\n",
            "58094\n",
            "58095\n",
            "58096\n",
            "58097\n",
            "58098\n",
            "58099\n",
            "58100\n",
            "58101\n",
            "58102\n",
            "58103\n",
            "58104\n",
            "58105\n",
            "58106\n",
            "58107\n",
            "58108\n",
            "58109\n",
            "58110\n",
            "58111\n",
            "58112\n",
            "58113\n",
            "58114\n",
            "58115\n",
            "58116\n",
            "58117\n",
            "58118\n",
            "58119\n",
            "58120\n",
            "58121\n",
            "58122\n",
            "58123\n",
            "58124\n",
            "58125\n",
            "58126\n",
            "58127\n",
            "58128\n",
            "58129\n",
            "58130\n",
            "58131\n",
            "58132\n",
            "58133\n",
            "58134\n",
            "58135\n",
            "58136\n",
            "58137\n",
            "58138\n",
            "58139\n",
            "58140\n",
            "58141\n",
            "58142\n",
            "58143\n",
            "58144\n",
            "58145\n",
            "58146\n",
            "58147\n",
            "58148\n",
            "58149\n",
            "58150\n",
            "58151\n",
            "58152\n",
            "58153\n",
            "58154\n",
            "58155\n",
            "58156\n",
            "58157\n",
            "58158\n",
            "58159\n",
            "58160\n",
            "58161\n",
            "58162\n",
            "58163\n",
            "58164\n",
            "58165\n",
            "58166\n",
            "58167\n",
            "58168\n",
            "58169\n",
            "58170\n",
            "58171\n",
            "58172\n",
            "58173\n",
            "58174\n",
            "58175\n",
            "58176\n",
            "58177\n",
            "58178\n",
            "58179\n",
            "58180\n",
            "58181\n",
            "58182\n",
            "58183\n",
            "58184\n",
            "58185\n",
            "58186\n",
            "58187\n",
            "58188\n",
            "58189\n",
            "58190\n",
            "58191\n",
            "58192\n",
            "58193\n",
            "58194\n",
            "58195\n",
            "58196\n",
            "58197\n",
            "58198\n",
            "58199\n",
            "58200\n",
            "58201\n",
            "58202\n",
            "58203\n",
            "58204\n",
            "58205\n",
            "58206\n",
            "58207\n",
            "58208\n",
            "58209\n",
            "58210\n",
            "58211\n",
            "58212\n",
            "58213\n",
            "58214\n",
            "58215\n",
            "58216\n",
            "58217\n",
            "58218\n",
            "58219\n",
            "58220\n",
            "58221\n",
            "58222\n",
            "58223\n",
            "58224\n",
            "58225\n",
            "58226\n",
            "58227\n",
            "58228\n",
            "58229\n",
            "58230\n",
            "58231\n",
            "58232\n",
            "58233\n",
            "58234\n",
            "58235\n",
            "58236\n",
            "58237\n",
            "58238\n",
            "58239\n",
            "58240\n",
            "58241\n",
            "58242\n",
            "58243\n",
            "58244\n",
            "58245\n",
            "58246\n",
            "58247\n",
            "58248\n",
            "58249\n",
            "58250\n",
            "58251\n",
            "58252\n",
            "58253\n",
            "58254\n",
            "58255\n",
            "58256\n",
            "58257\n",
            "58258\n",
            "58259\n",
            "58260\n",
            "58261\n",
            "58262\n",
            "58263\n",
            "58264\n",
            "58265\n",
            "58266\n",
            "58267\n",
            "58268\n",
            "58269\n",
            "58270\n",
            "58271\n",
            "58272\n",
            "58273\n",
            "58274\n",
            "58275\n",
            "58276\n",
            "58277\n",
            "58278\n",
            "58279\n",
            "58280\n",
            "58281\n",
            "58282\n",
            "58283\n",
            "58284\n",
            "58285\n",
            "58286\n",
            "58287\n",
            "58288\n",
            "58289\n",
            "58290\n",
            "58291\n",
            "58292\n",
            "58293\n",
            "58294\n",
            "58295\n",
            "58296\n",
            "58297\n",
            "58298\n",
            "58299\n",
            "58300\n",
            "58301\n",
            "58302\n",
            "58303\n",
            "58304\n",
            "58305\n",
            "58306\n",
            "58307\n",
            "58308\n",
            "58309\n",
            "58310\n",
            "58311\n",
            "58312\n",
            "58313\n",
            "58314\n",
            "58315\n",
            "58316\n",
            "58317\n",
            "58318\n",
            "58319\n",
            "58320\n",
            "58321\n",
            "58322\n",
            "58323\n",
            "58324\n",
            "58325\n",
            "58326\n",
            "58327\n",
            "58328\n",
            "58329\n",
            "58330\n",
            "58331\n",
            "58332\n",
            "58333\n",
            "58334\n",
            "58335\n",
            "58336\n",
            "58337\n",
            "58338\n",
            "58339\n",
            "58340\n",
            "58341\n",
            "58342\n",
            "58343\n",
            "58344\n",
            "58345\n",
            "58346\n",
            "58347\n",
            "58348\n",
            "58349\n",
            "58350\n",
            "58351\n",
            "58352\n",
            "58353\n",
            "58354\n",
            "58355\n",
            "58356\n",
            "58357\n",
            "58358\n",
            "58359\n",
            "58360\n",
            "58361\n",
            "58362\n",
            "58363\n",
            "58364\n",
            "58365\n",
            "58366\n",
            "58367\n",
            "58368\n",
            "58369\n",
            "58370\n",
            "58371\n",
            "58372\n",
            "58373\n",
            "58374\n",
            "58375\n",
            "58376\n",
            "58377\n",
            "58378\n",
            "58379\n",
            "58380\n",
            "58381\n",
            "58382\n",
            "58383\n",
            "58384\n",
            "58385\n",
            "58386\n",
            "58387\n",
            "58388\n",
            "58389\n",
            "58390\n",
            "58391\n",
            "58392\n",
            "58393\n",
            "58394\n",
            "58395\n",
            "58396\n",
            "58397\n",
            "58398\n",
            "58399\n",
            "58400\n",
            "58401\n",
            "58402\n",
            "58403\n",
            "58404\n",
            "58405\n",
            "58406\n",
            "58407\n",
            "58408\n",
            "58409\n",
            "58410\n",
            "58411\n",
            "58412\n",
            "58413\n",
            "58414\n",
            "58415\n",
            "58416\n",
            "58417\n",
            "58418\n",
            "58419\n",
            "58420\n",
            "58421\n",
            "58422\n",
            "58423\n",
            "58424\n",
            "58425\n",
            "58426\n",
            "58427\n",
            "58428\n",
            "58429\n",
            "58430\n",
            "58431\n",
            "58432\n",
            "58433\n",
            "58434\n",
            "58435\n",
            "58436\n",
            "58437\n",
            "58438\n",
            "58439\n",
            "58440\n",
            "58441\n",
            "58442\n",
            "58443\n",
            "58444\n",
            "58445\n",
            "58446\n",
            "58447\n",
            "58448\n",
            "58449\n",
            "58450\n",
            "58451\n",
            "58452\n",
            "58453\n",
            "58454\n",
            "58455\n",
            "58456\n",
            "58457\n",
            "58458\n",
            "58459\n",
            "58460\n",
            "58461\n",
            "58462\n",
            "58463\n",
            "58464\n",
            "58465\n",
            "58466\n",
            "58467\n",
            "58468\n",
            "58469\n",
            "58470\n",
            "58471\n",
            "58472\n",
            "58473\n",
            "58474\n",
            "58475\n",
            "58476\n",
            "58477\n",
            "58478\n",
            "58479\n",
            "58480\n",
            "58481\n",
            "58482\n",
            "58483\n",
            "58484\n",
            "58485\n",
            "58486\n",
            "58487\n",
            "58488\n",
            "58489\n",
            "58490\n",
            "58491\n",
            "58492\n",
            "58493\n",
            "58494\n",
            "58495\n",
            "58496\n",
            "58497\n",
            "58498\n",
            "58499\n",
            "58500\n",
            "58501\n",
            "58502\n",
            "58503\n",
            "58504\n",
            "58505\n",
            "58506\n",
            "58507\n",
            "58508\n",
            "58509\n",
            "58510\n",
            "58511\n",
            "58512\n",
            "58513\n",
            "58514\n",
            "58515\n",
            "58516\n",
            "58517\n",
            "58518\n",
            "58519\n",
            "58520\n",
            "58521\n",
            "58522\n",
            "58523\n",
            "58524\n",
            "58525\n",
            "58526\n",
            "58527\n",
            "58528\n",
            "58529\n",
            "58530\n",
            "58531\n",
            "58532\n",
            "58533\n",
            "58534\n",
            "58535\n",
            "58536\n",
            "58537\n",
            "58538\n",
            "58539\n",
            "58540\n",
            "58541\n",
            "58542\n",
            "58543\n",
            "58544\n",
            "58545\n",
            "58546\n",
            "58547\n",
            "58548\n",
            "58549\n",
            "58550\n",
            "58551\n",
            "58552\n",
            "58553\n",
            "58554\n",
            "58555\n",
            "58556\n",
            "58557\n",
            "58558\n",
            "58559\n",
            "58560\n",
            "58561\n",
            "58562\n",
            "58563\n",
            "58564\n",
            "58565\n",
            "58566\n",
            "58567\n",
            "58568\n",
            "58569\n",
            "58570\n",
            "58571\n",
            "58572\n",
            "58573\n",
            "58574\n",
            "58575\n",
            "58576\n",
            "58577\n",
            "58578\n",
            "58579\n",
            "58580\n",
            "58581\n",
            "58582\n",
            "58583\n",
            "58584\n",
            "58585\n",
            "58586\n",
            "58587\n",
            "58588\n",
            "58589\n",
            "58590\n",
            "58591\n",
            "58592\n",
            "58593\n",
            "58594\n",
            "58595\n",
            "58596\n",
            "58597\n",
            "58598\n",
            "58599\n",
            "58600\n",
            "58601\n",
            "58602\n",
            "58603\n",
            "58604\n",
            "58605\n",
            "58606\n",
            "58607\n",
            "58608\n",
            "58609\n",
            "58610\n",
            "58611\n",
            "58612\n",
            "58613\n",
            "58614\n",
            "58615\n",
            "58616\n",
            "58617\n",
            "58618\n",
            "58619\n",
            "58620\n",
            "58621\n",
            "58622\n",
            "58623\n",
            "58624\n",
            "58625\n",
            "58626\n",
            "58627\n",
            "58628\n",
            "58629\n",
            "58630\n",
            "58631\n",
            "58632\n",
            "58633\n",
            "58634\n",
            "58635\n",
            "58636\n",
            "58637\n",
            "58638\n",
            "58639\n",
            "58640\n",
            "58641\n",
            "58642\n",
            "58643\n",
            "58644\n",
            "58645\n",
            "58646\n",
            "58647\n",
            "58648\n",
            "58649\n",
            "58650\n",
            "58651\n",
            "58652\n",
            "58653\n",
            "58654\n",
            "58655\n",
            "58656\n",
            "58657\n",
            "58658\n",
            "58659\n",
            "58660\n",
            "58661\n",
            "58662\n",
            "58663\n",
            "58664\n",
            "58665\n",
            "58666\n",
            "58667\n",
            "58668\n",
            "58669\n",
            "58670\n",
            "58671\n",
            "58672\n",
            "58673\n",
            "58674\n",
            "58675\n",
            "58676\n",
            "58677\n",
            "58678\n",
            "58679\n",
            "58680\n",
            "58681\n",
            "58682\n",
            "58683\n",
            "58684\n",
            "58685\n",
            "58686\n",
            "58687\n",
            "58688\n",
            "58689\n",
            "58690\n",
            "58691\n",
            "58692\n",
            "58693\n",
            "58694\n",
            "58695\n",
            "58696\n",
            "58697\n",
            "58698\n",
            "58699\n",
            "58700\n",
            "58701\n",
            "58702\n",
            "58703\n",
            "58704\n",
            "58705\n",
            "58706\n",
            "58707\n",
            "58708\n",
            "58709\n",
            "58710\n",
            "58711\n",
            "58712\n",
            "58713\n",
            "58714\n",
            "58715\n",
            "58716\n",
            "58717\n",
            "58718\n",
            "58719\n",
            "58720\n",
            "58721\n",
            "58722\n",
            "58723\n",
            "58724\n",
            "58725\n",
            "58726\n",
            "58727\n",
            "58728\n",
            "58729\n",
            "58730\n",
            "58731\n",
            "58732\n",
            "58733\n",
            "58734\n",
            "58735\n",
            "58736\n",
            "58737\n",
            "58738\n",
            "58739\n",
            "58740\n",
            "58741\n",
            "58742\n",
            "58743\n",
            "58744\n",
            "58745\n",
            "58746\n",
            "58747\n",
            "58748\n",
            "58749\n",
            "58750\n",
            "58751\n",
            "58752\n",
            "58753\n",
            "58754\n",
            "58755\n",
            "58756\n",
            "58757\n",
            "58758\n",
            "58759\n",
            "58760\n",
            "58761\n",
            "58762\n",
            "58763\n",
            "58764\n",
            "58765\n",
            "58766\n",
            "58767\n",
            "58768\n",
            "58769\n",
            "58770\n",
            "58771\n",
            "58772\n",
            "58773\n",
            "58774\n",
            "58775\n",
            "58776\n",
            "58777\n",
            "58778\n",
            "58779\n",
            "58780\n",
            "58781\n",
            "58782\n",
            "58783\n",
            "58784\n",
            "58785\n",
            "58786\n",
            "58787\n",
            "58788\n",
            "58789\n",
            "58790\n",
            "58791\n",
            "58792\n",
            "58793\n",
            "58794\n",
            "58795\n",
            "58796\n",
            "58797\n",
            "58798\n",
            "58799\n",
            "58800\n",
            "58801\n",
            "58802\n",
            "58803\n",
            "58804\n",
            "58805\n",
            "58806\n",
            "58807\n",
            "58808\n",
            "58809\n",
            "58810\n",
            "58811\n",
            "58812\n",
            "58813\n",
            "58814\n",
            "58815\n",
            "58816\n",
            "58817\n",
            "58818\n",
            "58819\n",
            "58820\n",
            "58821\n",
            "58822\n",
            "58823\n",
            "58824\n",
            "58825\n",
            "58826\n",
            "58827\n",
            "58828\n",
            "58829\n",
            "58830\n",
            "58831\n",
            "58832\n",
            "58833\n",
            "58834\n",
            "58835\n",
            "58836\n",
            "58837\n",
            "58838\n",
            "58839\n",
            "58840\n",
            "58841\n",
            "58842\n",
            "58843\n",
            "58844\n",
            "58845\n",
            "58846\n",
            "58847\n",
            "58848\n",
            "58849\n",
            "58850\n",
            "58851\n",
            "58852\n",
            "58853\n",
            "58854\n",
            "58855\n",
            "58856\n",
            "58857\n",
            "58858\n",
            "58859\n",
            "58860\n",
            "58861\n",
            "58862\n",
            "58863\n",
            "58864\n",
            "58865\n",
            "58866\n",
            "58867\n",
            "58868\n",
            "58869\n",
            "58870\n",
            "58871\n",
            "58872\n",
            "58873\n",
            "58874\n",
            "58875\n",
            "58876\n",
            "58877\n",
            "58878\n",
            "58879\n",
            "58880\n",
            "58881\n",
            "58882\n",
            "58883\n",
            "58884\n",
            "58885\n",
            "58886\n",
            "58887\n",
            "58888\n",
            "58889\n",
            "58890\n",
            "58891\n",
            "58892\n",
            "58893\n",
            "58894\n",
            "58895\n",
            "58896\n",
            "58897\n",
            "58898\n",
            "58899\n",
            "58900\n",
            "58901\n",
            "58902\n",
            "58903\n",
            "58904\n",
            "58905\n",
            "58906\n",
            "58907\n",
            "58908\n",
            "58909\n",
            "58910\n",
            "58911\n",
            "58912\n",
            "58913\n",
            "58914\n",
            "58915\n",
            "58916\n",
            "58917\n",
            "58918\n",
            "58919\n",
            "58920\n",
            "58921\n",
            "58922\n",
            "58923\n",
            "58924\n",
            "58925\n",
            "58926\n",
            "58927\n",
            "58928\n",
            "58929\n",
            "58930\n",
            "58931\n",
            "58932\n",
            "58933\n",
            "58934\n",
            "58935\n",
            "58936\n",
            "58937\n",
            "58938\n",
            "58939\n",
            "58940\n",
            "58941\n",
            "58942\n",
            "58943\n",
            "58944\n",
            "58945\n",
            "58946\n",
            "58947\n",
            "58948\n",
            "58949\n",
            "58950\n",
            "58951\n",
            "58952\n",
            "58953\n",
            "58954\n",
            "58955\n",
            "58956\n",
            "58957\n",
            "58958\n",
            "58959\n",
            "58960\n",
            "58961\n",
            "58962\n",
            "58963\n",
            "58964\n",
            "58965\n",
            "58966\n",
            "58967\n",
            "58968\n",
            "58969\n",
            "58970\n",
            "58971\n",
            "58972\n",
            "58973\n",
            "58974\n",
            "58975\n",
            "58976\n",
            "58977\n",
            "58978\n",
            "58979\n",
            "58980\n",
            "58981\n",
            "58982\n",
            "58983\n",
            "58984\n",
            "58985\n",
            "58986\n",
            "58987\n",
            "58988\n",
            "58989\n",
            "58990\n",
            "58991\n",
            "58992\n",
            "58993\n",
            "58994\n",
            "58995\n",
            "58996\n",
            "58997\n",
            "58998\n",
            "58999\n",
            "59000\n",
            "59001\n",
            "59002\n",
            "59003\n",
            "59004\n",
            "59005\n",
            "59006\n",
            "59007\n",
            "59008\n",
            "59009\n",
            "59010\n",
            "59011\n",
            "59012\n",
            "59013\n",
            "59014\n",
            "59015\n",
            "59016\n",
            "59017\n",
            "59018\n",
            "59019\n",
            "59020\n",
            "59021\n",
            "59022\n",
            "59023\n",
            "59024\n",
            "59025\n",
            "59026\n",
            "59027\n",
            "59028\n",
            "59029\n",
            "59030\n",
            "59031\n",
            "59032\n",
            "59033\n",
            "59034\n",
            "59035\n",
            "59036\n",
            "59037\n",
            "59038\n",
            "59039\n",
            "59040\n",
            "59041\n",
            "59042\n",
            "59043\n",
            "59044\n",
            "59045\n",
            "59046\n",
            "59047\n",
            "59048\n",
            "59049\n",
            "59050\n",
            "59051\n",
            "59052\n",
            "59053\n",
            "59054\n",
            "59055\n",
            "59056\n",
            "59057\n",
            "59058\n",
            "59059\n",
            "59060\n",
            "59061\n",
            "59062\n",
            "59063\n",
            "59064\n",
            "59065\n",
            "59066\n",
            "59067\n",
            "59068\n",
            "59069\n",
            "59070\n",
            "59071\n",
            "59072\n",
            "59073\n",
            "59074\n",
            "59075\n",
            "59076\n",
            "59077\n",
            "59078\n",
            "59079\n",
            "59080\n",
            "59081\n",
            "59082\n",
            "59083\n",
            "59084\n",
            "59085\n",
            "59086\n",
            "59087\n",
            "59088\n",
            "59089\n",
            "59090\n",
            "59091\n",
            "59092\n",
            "59093\n",
            "59094\n",
            "59095\n",
            "59096\n",
            "59097\n",
            "59098\n",
            "59099\n",
            "59100\n",
            "59101\n",
            "59102\n",
            "59103\n",
            "59104\n",
            "59105\n",
            "59106\n",
            "59107\n",
            "59108\n",
            "59109\n",
            "59110\n",
            "59111\n",
            "59112\n",
            "59113\n",
            "59114\n",
            "59115\n",
            "59116\n",
            "59117\n",
            "59118\n",
            "59119\n",
            "59120\n",
            "59121\n",
            "59122\n",
            "59123\n",
            "59124\n",
            "59125\n",
            "59126\n",
            "59127\n",
            "59128\n",
            "59129\n",
            "59130\n",
            "59131\n",
            "59132\n",
            "59133\n",
            "59134\n",
            "59135\n",
            "59136\n",
            "59137\n",
            "59138\n",
            "59139\n",
            "59140\n",
            "59141\n",
            "59142\n",
            "59143\n",
            "59144\n",
            "59145\n",
            "59146\n",
            "59147\n",
            "59148\n",
            "59149\n",
            "59150\n",
            "59151\n",
            "59152\n",
            "59153\n",
            "59154\n",
            "59155\n",
            "59156\n",
            "59157\n",
            "59158\n",
            "59159\n",
            "59160\n",
            "59161\n",
            "59162\n",
            "59163\n",
            "59164\n",
            "59165\n",
            "59166\n",
            "59167\n",
            "59168\n",
            "59169\n",
            "59170\n",
            "59171\n",
            "59172\n",
            "59173\n",
            "59174\n",
            "59175\n",
            "59176\n",
            "59177\n",
            "59178\n",
            "59179\n",
            "59180\n",
            "59181\n",
            "59182\n",
            "59183\n",
            "59184\n",
            "59185\n",
            "59186\n",
            "59187\n",
            "59188\n",
            "59189\n",
            "59190\n",
            "59191\n",
            "59192\n",
            "59193\n",
            "59194\n",
            "59195\n",
            "59196\n",
            "59197\n",
            "59198\n",
            "59199\n",
            "59200\n",
            "59201\n",
            "59202\n",
            "59203\n",
            "59204\n",
            "59205\n",
            "59206\n",
            "59207\n",
            "59208\n",
            "59209\n",
            "59210\n",
            "59211\n",
            "59212\n",
            "59213\n",
            "59214\n",
            "59215\n",
            "59216\n",
            "59217\n",
            "59218\n",
            "59219\n",
            "59220\n",
            "59221\n",
            "59222\n",
            "59223\n",
            "59224\n",
            "59225\n",
            "59226\n",
            "59227\n",
            "59228\n",
            "59229\n",
            "59230\n",
            "59231\n",
            "59232\n",
            "59233\n",
            "59234\n",
            "59235\n",
            "59236\n",
            "59237\n",
            "59238\n",
            "59239\n",
            "59240\n",
            "59241\n",
            "59242\n",
            "59243\n",
            "59244\n",
            "59245\n",
            "59246\n",
            "59247\n",
            "59248\n",
            "59249\n",
            "59250\n",
            "59251\n",
            "59252\n",
            "59253\n",
            "59254\n",
            "59255\n",
            "59256\n",
            "59257\n",
            "59258\n",
            "59259\n",
            "59260\n",
            "59261\n",
            "59262\n",
            "59263\n",
            "59264\n",
            "59265\n",
            "59266\n",
            "59267\n",
            "59268\n",
            "59269\n",
            "59270\n",
            "59271\n",
            "59272\n",
            "59273\n",
            "59274\n",
            "59275\n",
            "59276\n",
            "59277\n",
            "59278\n",
            "59279\n",
            "59280\n",
            "59281\n",
            "59282\n",
            "59283\n",
            "59284\n",
            "59285\n",
            "59286\n",
            "59287\n",
            "59288\n",
            "59289\n",
            "59290\n",
            "59291\n",
            "59292\n",
            "59293\n",
            "59294\n",
            "59295\n",
            "59296\n",
            "59297\n",
            "59298\n",
            "59299\n",
            "59300\n",
            "59301\n",
            "59302\n",
            "59303\n",
            "59304\n",
            "59305\n",
            "59306\n",
            "59307\n",
            "59308\n",
            "59309\n",
            "59310\n",
            "59311\n",
            "59312\n",
            "59313\n",
            "59314\n",
            "59315\n",
            "59316\n",
            "59317\n",
            "59318\n",
            "59319\n",
            "59320\n",
            "59321\n",
            "59322\n",
            "59323\n",
            "59324\n",
            "59325\n",
            "59326\n",
            "59327\n",
            "59328\n",
            "59329\n",
            "59330\n",
            "59331\n",
            "59332\n",
            "59333\n",
            "59334\n",
            "59335\n",
            "59336\n",
            "59337\n",
            "59338\n",
            "59339\n",
            "59340\n",
            "59341\n",
            "59342\n",
            "59343\n",
            "59344\n",
            "59345\n",
            "59346\n",
            "59347\n",
            "59348\n",
            "59349\n",
            "59350\n",
            "59351\n",
            "59352\n",
            "59353\n",
            "59354\n",
            "59355\n",
            "59356\n",
            "59357\n",
            "59358\n",
            "59359\n",
            "59360\n",
            "59361\n",
            "59362\n",
            "59363\n",
            "59364\n",
            "59365\n",
            "59366\n",
            "59367\n",
            "59368\n",
            "59369\n",
            "59370\n",
            "59371\n",
            "59372\n",
            "59373\n",
            "59374\n",
            "59375\n",
            "59376\n",
            "59377\n",
            "59378\n",
            "59379\n",
            "59380\n",
            "59381\n",
            "59382\n",
            "59383\n",
            "59384\n",
            "59385\n",
            "59386\n",
            "59387\n",
            "59388\n",
            "59389\n",
            "59390\n",
            "59391\n",
            "59392\n",
            "59393\n",
            "59394\n",
            "59395\n",
            "59396\n",
            "59397\n",
            "59398\n",
            "59399\n",
            "59400\n",
            "59401\n",
            "59402\n",
            "59403\n",
            "59404\n",
            "59405\n",
            "59406\n",
            "59407\n",
            "59408\n",
            "59409\n",
            "59410\n",
            "59411\n",
            "59412\n",
            "59413\n",
            "59414\n",
            "59415\n",
            "59416\n",
            "59417\n",
            "59418\n",
            "59419\n",
            "59420\n",
            "59421\n",
            "59422\n",
            "59423\n",
            "59424\n",
            "59425\n",
            "59426\n",
            "59427\n",
            "59428\n",
            "59429\n",
            "59430\n",
            "59431\n",
            "59432\n",
            "59433\n",
            "59434\n",
            "59435\n",
            "59436\n",
            "59437\n",
            "59438\n",
            "59439\n",
            "59440\n",
            "59441\n",
            "59442\n",
            "59443\n",
            "59444\n",
            "59445\n",
            "59446\n",
            "59447\n",
            "59448\n",
            "59449\n",
            "59450\n",
            "59451\n",
            "59452\n",
            "59453\n",
            "59454\n",
            "59455\n",
            "59456\n",
            "59457\n",
            "59458\n",
            "59459\n",
            "59460\n",
            "59461\n",
            "59462\n",
            "59463\n",
            "59464\n",
            "59465\n",
            "59466\n",
            "59467\n",
            "59468\n",
            "59469\n",
            "59470\n",
            "59471\n",
            "59472\n",
            "59473\n",
            "59474\n",
            "59475\n",
            "59476\n",
            "59477\n",
            "59478\n",
            "59479\n",
            "59480\n",
            "59481\n",
            "59482\n",
            "59483\n",
            "59484\n",
            "59485\n",
            "59486\n",
            "59487\n",
            "59488\n",
            "59489\n",
            "59490\n",
            "59491\n",
            "59492\n",
            "59493\n",
            "59494\n",
            "59495\n",
            "59496\n",
            "59497\n",
            "59498\n",
            "59499\n",
            "59500\n",
            "59501\n",
            "59502\n",
            "59503\n",
            "59504\n",
            "59505\n",
            "59506\n",
            "59507\n",
            "59508\n",
            "59509\n",
            "59510\n",
            "59511\n",
            "59512\n",
            "59513\n",
            "59514\n",
            "59515\n",
            "59516\n",
            "59517\n",
            "59518\n",
            "59519\n",
            "59520\n",
            "59521\n",
            "59522\n",
            "59523\n",
            "59524\n",
            "59525\n",
            "59526\n",
            "59527\n",
            "59528\n",
            "59529\n",
            "59530\n",
            "59531\n",
            "59532\n",
            "59533\n",
            "59534\n",
            "59535\n",
            "59536\n",
            "59537\n",
            "59538\n",
            "59539\n",
            "59540\n",
            "59541\n",
            "59542\n",
            "59543\n",
            "59544\n",
            "59545\n",
            "59546\n",
            "59547\n",
            "59548\n",
            "59549\n",
            "59550\n",
            "59551\n",
            "59552\n",
            "59553\n",
            "59554\n",
            "59555\n",
            "59556\n",
            "59557\n",
            "59558\n",
            "59559\n",
            "59560\n",
            "59561\n",
            "59562\n",
            "59563\n",
            "59564\n",
            "59565\n",
            "59566\n",
            "59567\n",
            "59568\n",
            "59569\n",
            "59570\n",
            "59571\n",
            "59572\n",
            "59573\n",
            "59574\n",
            "59575\n",
            "59576\n",
            "59577\n",
            "59578\n",
            "59579\n",
            "59580\n",
            "59581\n",
            "59582\n",
            "59583\n",
            "59584\n",
            "59585\n",
            "59586\n",
            "59587\n",
            "59588\n",
            "59589\n",
            "59590\n",
            "59591\n",
            "59592\n",
            "59593\n",
            "59594\n",
            "59595\n",
            "59596\n",
            "59597\n",
            "59598\n",
            "59599\n",
            "59600\n",
            "59601\n",
            "59602\n",
            "59603\n",
            "59604\n",
            "59605\n",
            "59606\n",
            "59607\n",
            "59608\n",
            "59609\n",
            "59610\n",
            "59611\n",
            "59612\n",
            "59613\n",
            "59614\n",
            "59615\n",
            "59616\n",
            "59617\n",
            "59618\n",
            "59619\n",
            "59620\n",
            "59621\n",
            "59622\n",
            "59623\n",
            "59624\n",
            "59625\n",
            "59626\n",
            "59627\n",
            "59628\n",
            "59629\n",
            "59630\n",
            "59631\n",
            "59632\n",
            "59633\n",
            "59634\n",
            "59635\n",
            "59636\n",
            "59637\n",
            "59638\n",
            "59639\n",
            "59640\n",
            "59641\n",
            "59642\n",
            "59643\n",
            "59644\n",
            "59645\n",
            "59646\n",
            "59647\n",
            "59648\n",
            "59649\n",
            "59650\n",
            "59651\n",
            "59652\n",
            "59653\n",
            "59654\n",
            "59655\n",
            "59656\n",
            "59657\n",
            "59658\n",
            "59659\n",
            "59660\n",
            "59661\n",
            "59662\n",
            "59663\n",
            "59664\n",
            "59665\n",
            "59666\n",
            "59667\n",
            "59668\n",
            "59669\n",
            "59670\n",
            "59671\n",
            "59672\n",
            "59673\n",
            "59674\n",
            "59675\n",
            "59676\n",
            "59677\n",
            "59678\n",
            "59679\n",
            "59680\n",
            "59681\n",
            "59682\n",
            "59683\n",
            "59684\n",
            "59685\n",
            "59686\n",
            "59687\n",
            "59688\n",
            "59689\n",
            "59690\n",
            "59691\n",
            "59692\n",
            "59693\n",
            "59694\n",
            "59695\n",
            "59696\n",
            "59697\n",
            "59698\n",
            "59699\n",
            "59700\n",
            "59701\n",
            "59702\n",
            "59703\n",
            "59704\n",
            "59705\n",
            "59706\n",
            "59707\n",
            "59708\n",
            "59709\n",
            "59710\n",
            "59711\n",
            "59712\n",
            "59713\n",
            "59714\n",
            "59715\n",
            "59716\n",
            "59717\n",
            "59718\n",
            "59719\n",
            "59720\n",
            "59721\n",
            "59722\n",
            "59723\n",
            "59724\n",
            "59725\n",
            "59726\n",
            "59727\n",
            "59728\n",
            "59729\n",
            "59730\n",
            "59731\n",
            "59732\n",
            "59733\n",
            "59734\n",
            "59735\n",
            "59736\n",
            "59737\n",
            "59738\n",
            "59739\n",
            "59740\n",
            "59741\n",
            "59742\n",
            "59743\n",
            "59744\n",
            "59745\n",
            "59746\n",
            "59747\n",
            "59748\n",
            "59749\n",
            "59750\n",
            "59751\n",
            "59752\n",
            "59753\n",
            "59754\n",
            "59755\n",
            "59756\n",
            "59757\n",
            "59758\n",
            "59759\n",
            "59760\n",
            "59761\n",
            "59762\n",
            "59763\n",
            "59764\n",
            "59765\n",
            "59766\n",
            "59767\n",
            "59768\n",
            "59769\n",
            "59770\n",
            "59771\n",
            "59772\n",
            "59773\n",
            "59774\n",
            "59775\n",
            "59776\n",
            "59777\n",
            "59778\n",
            "59779\n",
            "59780\n",
            "59781\n",
            "59782\n",
            "59783\n",
            "59784\n",
            "59785\n",
            "59786\n",
            "59787\n",
            "59788\n",
            "59789\n",
            "59790\n",
            "59791\n",
            "59792\n",
            "59793\n",
            "59794\n",
            "59795\n",
            "59796\n",
            "59797\n",
            "59798\n",
            "59799\n",
            "59800\n",
            "59801\n",
            "59802\n",
            "59803\n",
            "59804\n",
            "59805\n",
            "59806\n",
            "59807\n",
            "59808\n",
            "59809\n",
            "59810\n",
            "59811\n",
            "59812\n",
            "59813\n",
            "59814\n",
            "59815\n",
            "59816\n",
            "59817\n",
            "59818\n",
            "59819\n",
            "59820\n",
            "59821\n",
            "59822\n",
            "59823\n",
            "59824\n",
            "59825\n",
            "59826\n",
            "59827\n",
            "59828\n",
            "59829\n",
            "59830\n",
            "59831\n",
            "59832\n",
            "59833\n",
            "59834\n",
            "59835\n",
            "59836\n",
            "59837\n",
            "59838\n",
            "59839\n",
            "59840\n",
            "59841\n",
            "59842\n",
            "59843\n",
            "59844\n",
            "59845\n",
            "59846\n",
            "59847\n",
            "59848\n",
            "59849\n",
            "59850\n",
            "59851\n",
            "59852\n",
            "59853\n",
            "59854\n",
            "59855\n",
            "59856\n",
            "59857\n",
            "59858\n",
            "59859\n",
            "59860\n",
            "59861\n",
            "59862\n",
            "59863\n",
            "59864\n",
            "59865\n",
            "59866\n",
            "59867\n",
            "59868\n",
            "59869\n",
            "59870\n",
            "59871\n",
            "59872\n",
            "59873\n",
            "59874\n",
            "59875\n",
            "59876\n",
            "59877\n",
            "59878\n",
            "59879\n",
            "59880\n",
            "59881\n",
            "59882\n",
            "59883\n",
            "59884\n",
            "59885\n",
            "59886\n",
            "59887\n",
            "59888\n",
            "59889\n",
            "59890\n",
            "59891\n",
            "59892\n",
            "59893\n",
            "59894\n",
            "59895\n",
            "59896\n",
            "59897\n",
            "59898\n",
            "59899\n",
            "59900\n",
            "59901\n",
            "59902\n",
            "59903\n",
            "59904\n",
            "59905\n",
            "59906\n",
            "59907\n",
            "59908\n",
            "59909\n",
            "59910\n",
            "59911\n",
            "59912\n",
            "59913\n",
            "59914\n",
            "59915\n",
            "59916\n",
            "59917\n",
            "59918\n",
            "59919\n",
            "59920\n",
            "59921\n",
            "59922\n",
            "59923\n",
            "59924\n",
            "59925\n",
            "59926\n",
            "59927\n",
            "59928\n",
            "59929\n",
            "59930\n",
            "59931\n",
            "59932\n",
            "59933\n",
            "59934\n",
            "59935\n",
            "59936\n",
            "59937\n",
            "59938\n",
            "59939\n",
            "59940\n",
            "59941\n",
            "59942\n",
            "59943\n",
            "59944\n",
            "59945\n",
            "59946\n",
            "59947\n",
            "59948\n",
            "59949\n",
            "59950\n",
            "59951\n",
            "59952\n",
            "59953\n",
            "59954\n",
            "59955\n",
            "59956\n",
            "59957\n",
            "59958\n",
            "59959\n",
            "59960\n",
            "59961\n",
            "59962\n",
            "59963\n",
            "59964\n",
            "59965\n",
            "59966\n",
            "59967\n",
            "59968\n",
            "59969\n",
            "59970\n",
            "59971\n",
            "59972\n",
            "59973\n",
            "59974\n",
            "59975\n",
            "59976\n",
            "59977\n",
            "59978\n",
            "59979\n",
            "59980\n",
            "59981\n",
            "59982\n",
            "59983\n",
            "59984\n",
            "59985\n",
            "59986\n",
            "59987\n",
            "59988\n",
            "59989\n",
            "59990\n",
            "59991\n",
            "59992\n",
            "59993\n",
            "59994\n",
            "59995\n",
            "59996\n",
            "59997\n",
            "59998\n",
            "59999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Unet on MNIST data\n"
      ],
      "metadata": {
        "id": "l2x1A1GfT6nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we pass in where we want to store our model, where we want to load it from, and where our data is, and then train our UNet to denoise."
      ],
      "metadata": {
        "id": "u4R_PRhtY63g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "437fEsVY0zAr",
        "outputId": "2b4a1e55-3bfa-45a4-e599-e5be35f626a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_steps = 20\n",
        "blur_size = 11\n",
        "blur_std = 7.0\n",
        "blur_routine = 'Constant'\n",
        "sampling_routine = 'x0_step_down'\n",
        "discrete = True\n",
        "train_routine = 'Final'\n",
        "data_path = 'content/MyDrive/CS4782/Data'\n",
        "save_folder = 'content/MyDrive/CS4782/Results'\n",
        "load_path = 'content/MyDrive/CS4782/Results/Model.pt'\n",
        "results_path = 'content/MyDrive/CS4782/Results'\n",
        "\n",
        "# Total training steps\n",
        "train_steps = 10000\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--time_steps', default=50, type=int,\n",
        "#                     help=\"This is the number of steps in which a clean image looses information.\")\n",
        "# parser.add_argument('--train_steps', default=700000, type=int,\n",
        "#                     help='The number of iterations for training.')\n",
        "# parser.add_argument('--blur_std', default=0.1, type=float,\n",
        "#                     help='It sets the standard deviation for blur routines which have different meaning based on blur routine.')\n",
        "# parser.add_argument('--blur_size', default=3, type=int,\n",
        "#                     help='It sets the size of gaussian blur used in blur routines for each step t')\n",
        "# parser.add_argument('--save_folder', default='./results_mnist', type=str)\n",
        "# parser.add_argument('--data_path', default='./root_mnist/', type=str)\n",
        "# parser.add_argument('--load_path', default=None, type=str)\n",
        "# parser.add_argument('--blur_routine', default='Incremental', type=str,\n",
        "#                     help='This will set the type of blur routine one can use, check the code for what each one of them does in detail')\n",
        "# parser.add_argument('--train_routine', default='Final', type=str)\n",
        "# parser.add_argument('--sampling_routine', default='x0_step_down', type=str,\n",
        "#                     help='The choice of sampling routine for reversing the diffusion process, when set as default it corresponds to Alg. 1 while when set as x0_step_down it stands for Alg. 2')\n",
        "# parser.add_argument('--discrete', action=\"store_true\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# print(args)\n",
        "\n",
        "\n",
        "model = Unet(\n",
        "    dim = 64,\n",
        "    dim_mults = (1, 2, 4, 8),\n",
        "    channels=1\n",
        ").cuda()\n",
        "\n",
        "\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size = 32,\n",
        "    device_of_kernel = 'cuda',\n",
        "    channels = 1,\n",
        "    timesteps = time_steps,   # number of steps\n",
        "    loss_type = 'l1',    # L1 or L2\n",
        "    kernel_std= blur_std,\n",
        "    kernel_size= blur_size,\n",
        "    blur_routine= blur_routine,\n",
        "    train_routine = train_routine,\n",
        "    sampling_routine = sampling_routine,\n",
        "    discrete= discrete\n",
        ").cuda()\n",
        "\n",
        "diffusion = torch.nn.DataParallel(diffusion, device_ids=range(torch.cuda.device_count()))\n"
      ],
      "metadata": {
        "id": "0bw98eXYmE2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da727465-ec71-4fae-dc70-a4fe1060dedc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Time embed used ?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_folder = 'content/MyDrive/CS4782/Results/model.pt'\n",
        "train_steps = 100000\n",
        "\n",
        "trainer = Trainer(\n",
        "    diffusion,\n",
        "    data_path,\n",
        "    image_size = 32,\n",
        "    train_batch_size = 32,\n",
        "    train_lr = 2e-5,\n",
        "    train_num_steps = train_steps,         # total training steps\n",
        "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
        "    ema_decay = 0.995,                # exponential moving average decay\n",
        "    fp16 = False,                       # turn on mixed precision training with apex\n",
        "    results_folder = results_path,\n",
        "    load_path = None,\n",
        "    dataset = 'mnist',\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 1000 time steps 3 minutes on A100\n",
        "# 10000 time steps on A1000: ~ 1hr\n",
        "# 100000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lHuJgoVXHt24",
        "outputId": "3aa4fc89-ae5f-4172-f990-cbd9f0a490bb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnist DA used\n",
            "0: 1.0301005840301514\n",
            "0: 1.0342535972595215\n",
            "100: 0.3057045340538025\n",
            "100: 0.3170812726020813\n",
            "200: 0.28413599729537964\n",
            "200: 0.2834038734436035\n",
            "300: 0.2744976878166199\n",
            "300: 0.2505345940589905\n",
            "400: 0.24692560732364655\n",
            "400: 0.24047085642814636\n",
            "500: 0.22468923032283783\n",
            "500: 0.22170810401439667\n",
            "600: 0.2145889699459076\n",
            "600: 0.2319849729537964\n",
            "700: 0.1958080381155014\n",
            "700: 0.20537494122982025\n",
            "800: 0.22586405277252197\n",
            "800: 0.2064579874277115\n",
            "900: 0.19664177298545837\n",
            "900: 0.19134396314620972\n",
            "1000: 0.1790616363286972\n",
            "1000: 0.19633343815803528\n",
            "tensor(-0.5552, device='cuda:0')\n",
            "Mean of last 1000: 0.24666448250844644\n",
            "1100: 0.18080547451972961\n",
            "1100: 0.17280128598213196\n",
            "1200: 0.18772989511489868\n",
            "1200: 0.17022769153118134\n",
            "1300: 0.17819082736968994\n",
            "1300: 0.19059962034225464\n",
            "1400: 0.17892827093601227\n",
            "1400: 0.16035185754299164\n",
            "1500: 0.1899210661649704\n",
            "1500: 0.1831151396036148\n",
            "1600: 0.17386798560619354\n",
            "1600: 0.16745567321777344\n",
            "1700: 0.18637728691101074\n",
            "1700: 0.1423674076795578\n",
            "1800: 0.19378766417503357\n",
            "1800: 0.17284761369228363\n",
            "1900: 0.15976279973983765\n",
            "1900: 0.17179250717163086\n",
            "2000: 0.17926417291164398\n",
            "2000: 0.16237328946590424\n",
            "tensor(-0.6084, device='cuda:0')\n",
            "Mean of last 2000: 0.17791764418800154\n",
            "2100: 0.16247206926345825\n",
            "2100: 0.16185902059078217\n",
            "2200: 0.1775183230638504\n",
            "2200: 0.17728561162948608\n",
            "2300: 0.15201599895954132\n",
            "2300: 0.17305855453014374\n",
            "2400: 0.19026288390159607\n",
            "2400: 0.19107255339622498\n",
            "2500: 0.17515729367733002\n",
            "2500: 0.15615223348140717\n",
            "2600: 0.18032287061214447\n",
            "2600: 0.15196478366851807\n",
            "2700: 0.1706775575876236\n",
            "2700: 0.1641906499862671\n",
            "2800: 0.13840793073177338\n",
            "2800: 0.13402944803237915\n",
            "2900: 0.1355379968881607\n",
            "2900: 0.1326747089624405\n",
            "3000: 0.1631280928850174\n",
            "3000: 0.14937308430671692\n",
            "tensor(-0.6851, device='cuda:0')\n",
            "Mean of last 3000: 0.16355945975273162\n",
            "3100: 0.17188063263893127\n",
            "3100: 0.15624740719795227\n",
            "3200: 0.18675877153873444\n",
            "3200: 0.16091924905776978\n",
            "3300: 0.17070691287517548\n",
            "3300: 0.14810650050640106\n",
            "3400: 0.15115365386009216\n",
            "3400: 0.14784255623817444\n",
            "3500: 0.1493431180715561\n",
            "3500: 0.18246665596961975\n",
            "3600: 0.14890363812446594\n",
            "3600: 0.14392998814582825\n",
            "3700: 0.15630200505256653\n",
            "3700: 0.15572448074817657\n",
            "3800: 0.15063327550888062\n",
            "3800: 0.17297899723052979\n",
            "3900: 0.1456560492515564\n",
            "3900: 0.15030574798583984\n",
            "4000: 0.1529608964920044\n",
            "4000: 0.16292041540145874\n",
            "tensor(-0.8690, device='cuda:0')\n",
            "Mean of last 4000: 0.15645656082313972\n",
            "4100: 0.15132975578308105\n",
            "4100: 0.1330929696559906\n",
            "4200: 0.17002971470355988\n",
            "4200: 0.1457577794790268\n",
            "4300: 0.1581716388463974\n",
            "4300: 0.146841362118721\n",
            "4400: 0.15686261653900146\n",
            "4400: 0.16387400031089783\n",
            "4500: 0.130826935172081\n",
            "4500: 0.1531386375427246\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b1c15d4992de>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 1000 time steps 3 minutes on A100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-d810a9e1ccee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.step}: {loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mu_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mbackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulate_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "3sX3dO1Vpx3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we test our model. The results of this testing are stored in the 'content/MyDrive/CS4782/Results' folder."
      ],
      "metadata": {
        "id": "3wTZ0YnzZQA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test_from_data('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "zjgzXCkS7onw",
        "outputId": "f39fbf88-0401-40e9-cf66-3d43b92b0cf8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-d810a9e1ccee>:205: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  frames_0.append(imageio.imread(str(self.results_folder / f'sample-{i}-{extra_path}-x0.png')))\n",
            "<ipython-input-51-d810a9e1ccee>:211: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  frames_t.append(imageio.imread(str(self.results_folder / f'sample-{i}-{extra_path}-xt.png')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5b9d3139b8c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_from_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-d810a9e1ccee>\u001b[0m in \u001b[0;36mtest_from_data\u001b[0;34m(self, extra_path, s_times)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mframes_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'sample-{i}-{extra_path}-x0.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mall_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'sample-{i}-{extra_path}-xt.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.paper_showing_diffusion_images_cover_page_both_sampling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FtiLS1-iOwAz",
        "outputId": "a7a69652-7163-4e5d-dd8f-c6fbb347a351"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "torch.Size([32, 1, 32, 32])\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846) exited unexpectedly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-051628731227>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaper_showing_diffusion_images_cover_page_both_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-d810a9e1ccee>\u001b[0m in \u001b[0;36mpaper_showing_diffusion_images_cover_page_both_sampling\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mog_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mog_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6d59f4fe5432>\u001b[0m in \u001b[0;36mcycle\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'DataLoader worker (pid(s) {pids_str}) exited unexpectedly'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846) exited unexpectedly"
          ]
        }
      ]
    }
  ]
}